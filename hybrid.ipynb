{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168e220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Manual Gradients on 43 parameters.\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 1: Loss=0.100 | AUC=0.500\n",
      "  >>> New Best AUC! Model Saved.\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 2: Loss=0.100 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 3: Loss=0.200 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 4: Loss=0.100 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 5: Loss=0.100 | AUC=0.500\n",
      "\n",
      "Final Evaluation on Test Set...\n",
      "Final Test ROC AUC: 0.6250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, roc_curve\n",
    ")\n",
    "\n",
    "# System and Warning Handling\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA GENERATION (Simulated Fraud Data)\n",
    "# ==========================================\n",
    "def get_data():\n",
    "    # Reduced features to 4 for speed (Manual Gradients are slow!)\n",
    "    X, y = make_classification(n_samples=300, n_features=4, n_informative=4, \n",
    "                               n_redundant=0, weights=[0.9, 0.1], random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data()\n",
    "\n",
    "# ==========================================\n",
    "# 2. HYBRID MODEL (Forward Pass)\n",
    "# ==========================================\n",
    "def apply_kerr(circuit, kerr_param, qumode, cutoff):\n",
    "    \"\"\"Manual Kerr Gate using SNAP\"\"\"\n",
    "    for n in range(cutoff):\n",
    "        phase = kerr_param * (n ** 2)\n",
    "        if abs(phase) > 1e-4:\n",
    "            circuit.cv_snap(phase, n, qumode, None)\n",
    "\n",
    "def get_probs(result):\n",
    "    \"\"\"Extract probabilities for Class 0 (|1,0>) and Class 1 (|0,1>)\"\"\"\n",
    "    counts = result.get_counts()\n",
    "    total_shots = sum(counts.values())\n",
    "    p_10, p_01 = 0, 0\n",
    "    \n",
    "    for bitstr, count in counts.items():\n",
    "        val = int(bitstr.replace(\" \", \"\"), 2)\n",
    "        # Assuming 2 qubits per mode (Cutoff=4)\n",
    "        n_a = val & 3\n",
    "        n_b = (val >> 2) & 3\n",
    "        \n",
    "        if n_a == 1 and n_b == 0: p_10 += count\n",
    "        elif n_a == 0 and n_b == 1: p_01 += count\n",
    "            \n",
    "    return (p_10/total_shots, p_01/total_shots) if total_shots > 0 else (0,0)\n",
    "\n",
    "def hybrid_forward(params, x_input):\n",
    "    # --- Classical NN (4 inputs -> 8 outputs) ---\n",
    "    # Reduced size for speed\n",
    "    n_in, n_out = 4, 8\n",
    "    n_classical = n_in * n_out + n_out\n",
    "    \n",
    "    w = params[:n_in*n_out].reshape(n_in, n_out)\n",
    "    b = params[n_in*n_out : n_classical]\n",
    "    \n",
    "    # NN Output used as parameters for the encoding layer\n",
    "    nn_out = np.tanh(np.dot(x_input, w) + b)\n",
    "    \n",
    "    # --- Quantum Circuit ---\n",
    "    qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=2)\n",
    "    cr = qiskit.ClassicalRegister(4)\n",
    "    circuit = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "    \n",
    "    # Encoding (Driven by NN output)\n",
    "    # Using first 8 params for: Sq(0), Sq(1), BS, R(0), R(1), D(0), D(1), Kerr(0)\n",
    "    circuit.cv_sq(nn_out[0], qmr[0])\n",
    "    circuit.cv_sq(nn_out[1], qmr[1])\n",
    "    circuit.cv_bs(nn_out[2], qmr[0], qmr[1])\n",
    "    circuit.cv_r(nn_out[3], qmr[0])\n",
    "    circuit.cv_d(nn_out[4], qmr[0])\n",
    "    apply_kerr(circuit, nn_out[5], qmr[0], 4)\n",
    "    \n",
    "    # Variational Layer (Trainable Quantum Params)\n",
    "    q_params = params[n_classical:]\n",
    "    circuit.cv_bs(q_params[0], qmr[0], qmr[1])\n",
    "    circuit.cv_r(q_params[1], qmr[0])\n",
    "    circuit.cv_snap(q_params[2], 1, qmr[0], None) # Non-linearity\n",
    "\n",
    "    circuit.cv_measure(qmr, cr)\n",
    "    \n",
    "    # Simulate\n",
    "    try:\n",
    "        _, res, *_ = bosonic_qiskit.util.simulate(circuit, shots=100)\n",
    "        return get_probs(res)\n",
    "    except:\n",
    "        return 0.5, 0.5\n",
    "\n",
    "# ==========================================\n",
    "# 3. MANUAL GRADIENT CALCULATION\n",
    "# ==========================================\n",
    "def get_batch_loss(params, X_batch, y_batch):\n",
    "    loss = 0\n",
    "    scores = []\n",
    "    \n",
    "    for x, y in zip(X_batch, y_batch):\n",
    "        p_10, p_01 = hybrid_forward(params, x)\n",
    "        \n",
    "        # Loss: MSE distance from ideal probability\n",
    "        # Class 0 -> Target p_10=1.0 (p_01=0.0)\n",
    "        # Class 1 -> Target p_01=1.0 \n",
    "        target = 1.0 if y == 1 else 0.0\n",
    "        pred = p_01\n",
    "        \n",
    "        loss += (pred - target) ** 2\n",
    "        scores.append(pred) # Score for AUC is p_01 (Fraud Probability)\n",
    "        \n",
    "    return loss / len(y_batch), scores\n",
    "\n",
    "def compute_gradients_finite_diff(params, X_batch, y_batch, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Manually calculates gradient for EACH parameter.\n",
    "    Run time = 2 * N_params * Batch_Size * Simulator_Time\n",
    "    \"\"\"\n",
    "    grads = np.zeros_like(params)\n",
    "    \n",
    "    # Base loss (optional, for debugging)\n",
    "    # base_loss, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "    \n",
    "    print(f\"  Computing Gradients for {len(params)} parameters...\", end=\"\", flush=True)\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        # Shift Right\n",
    "        params[i] += epsilon\n",
    "        loss_plus, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "        \n",
    "        # Shift Left\n",
    "        params[i] -= 2 * epsilon # Move back to -epsilon\n",
    "        loss_minus, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "        \n",
    "        # Reset\n",
    "        params[i] += epsilon \n",
    "        \n",
    "        # Central Difference\n",
    "        grads[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        if i % 10 == 0: print(\".\", end=\"\", flush=True)\n",
    "            \n",
    "    print(\" Done.\")\n",
    "    return grads\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP (MAXIMIZE AUC)\n",
    "# ==========================================\n",
    "# Setup Parameters: 4*8 (Weights) + 8 (Bias) + 3 (Quantum) = 43 params\n",
    "n_params = (4 * 8) + 8 + 3\n",
    "weights = np.random.normal(0, 0.1, n_params)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 5 # Small number due to simulation time\n",
    "batch_size = 4 \n",
    "\n",
    "best_val_auc = 0\n",
    "best_weights = None\n",
    "\n",
    "print(f\"Starting Training with Manual Gradients on {n_params} parameters.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Mini-Batch Training\n",
    "    indices = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "    X_batch = X_train[indices]\n",
    "    y_batch = y_train[indices]\n",
    "    \n",
    "    # 2. Compute Manual Gradients\n",
    "    grads = compute_gradients_finite_diff(weights, X_batch, y_batch)\n",
    "    \n",
    "    # 3. Update Weights (Gradient Descent)\n",
    "    weights = weights - (learning_rate * grads)\n",
    "    \n",
    "    # 4. Validation & AUC Check\n",
    "    # We evaluate on a hold-out set to find the Best AUC model\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    val_indices = np.random.choice(len(X_test), 10, replace=False) # Subset for speed\n",
    "    \n",
    "    for idx in val_indices:\n",
    "        p_10, p_01 = hybrid_forward(weights, X_test[idx])\n",
    "        val_preds.append(p_01)\n",
    "        val_loss += (p_01 - (1 if y_test[idx]==1 else 0))**2\n",
    "        \n",
    "    try:\n",
    "        # Calculate AUC (needs both classes present in the batch)\n",
    "        if len(set(y_test[val_indices])) > 1:\n",
    "            current_auc = roc_auc_score(y_test[val_indices], val_preds)\n",
    "        else:\n",
    "            current_auc = 0.5\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={val_loss/10:.3f} | AUC={current_auc:.3f}\")\n",
    "        \n",
    "        # SAVE THE MODEL WITH BEST AUC\n",
    "        if current_auc > best_val_auc:\n",
    "            best_val_auc = current_auc\n",
    "            best_weights = weights.copy()\n",
    "            print(\"  >>> New Best AUC! Model Saved.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Metric Calc Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FINAL TEST\n",
    "# ==========================================\n",
    "print(\"\\nFinal Evaluation on Test Set...\")\n",
    "final_preds = []\n",
    "for x in X_test[:30]: # Limit for speed\n",
    "    _, p_01 = hybrid_forward(best_weights if best_weights is not None else weights, x)\n",
    "    final_preds.append(p_01)\n",
    "\n",
    "final_auc = roc_auc_score(y_test[:30], final_preds)\n",
    "print(f\"Final Test ROC AUC: {final_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 3,
   "id": "dc795c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0000\n",
      "Loss:       1.0000\n",
      "Grads:      Sq=0.0000, Rot=0.0000\n",
      "New Params: Sq=0.1000, Rot=0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "\n",
    "# --- 1. The Micrograd Engine (Minimal Version) ---\n",
    "class Value:\n",
    "    \"\"\"Stores a scalar and its gradient.\"\"\"\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "# --- 2. The Differentiable Gate Operation ---\n",
    "class QuantumOp(Value):\n",
    "    \"\"\"\n",
    "    Represents the RESULT of a quantum circuit execution.\n",
    "    It connects the Gate Parameters (parents) to the Measurement (output).\n",
    "    \"\"\"\n",
    "    def __init__(self, circuit_builder, params, input_val):\n",
    "        self.params = params  # List of Value objects (The Gate Angles)\n",
    "        self.input_val = input_val\n",
    "        self.circuit_builder = circuit_builder\n",
    "        \n",
    "        # Forward Pass: Run the circuit once to get the value\n",
    "        val = self._run_simulation([p.data for p in self.params])\n",
    "        \n",
    "        super().__init__(val, tuple(params), 'QuantumGate')\n",
    "\n",
    "    def _run_simulation(self, numeric_params):\n",
    "        \"\"\"Helper to run the actual simulator\"\"\"\n",
    "        # Rebuild circuit with specific numeric values\n",
    "        qmr = bosonic_qiskit.QumodeRegister(1, 2)\n",
    "        cr = qiskit.ClassicalRegister(2)\n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "        \n",
    "        # Call the user's circuit definition\n",
    "        self.circuit_builder(circ, numeric_params, self.input_val, qmr)\n",
    "        \n",
    "        circ.cv_measure(qmr, cr)\n",
    "        # Suppress output, run sim\n",
    "        try:\n",
    "            _, res, *_ = bosonic_qiskit.util.simulate(circ, shots=100)\n",
    "            counts = res.get_counts()\n",
    "            # Return Prob(|1>)\n",
    "            return counts.get('1', 0) / 100.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _backward_step(self):\n",
    "        \"\"\"The Parameter Shift Rule Implementation\"\"\"\n",
    "        shift = np.pi / 2\n",
    "        \n",
    "        for i, param in enumerate(self.params):\n",
    "            # 1. Shift +\n",
    "            p_vals = [p.data for p in self.params]\n",
    "            p_vals[i] += shift\n",
    "            out_plus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # 2. Shift -\n",
    "            p_vals[i] -= 2 * shift # Go from +s to -s\n",
    "            out_minus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # 3. Gradient\n",
    "            local_grad = (out_plus - out_minus) / 2.0\n",
    "            \n",
    "            # Chain Rule\n",
    "            param.grad += local_grad * self.grad\n",
    "\n",
    "# --- 3. User Code: Defining the Circuit ---\n",
    "\n",
    "def my_quantum_layer(circuit, numeric_params, x_input, qmr):\n",
    "    \"\"\"\n",
    "    The physical definition of the circuit.\n",
    "    numeric_params[0] -> Squeezing\n",
    "    numeric_params[1] -> Rotation\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    circuit.cv_d(x_input, qmr[0])\n",
    "    \n",
    "    # Variational Gates\n",
    "    circuit.cv_sq(numeric_params[0], qmr[0]) \n",
    "    circuit.cv_r(numeric_params[1], qmr[0])\n",
    "\n",
    "# --- 4. Training Script ---\n",
    "\n",
    "# Init Trainable Weights\n",
    "w_squeeze = Value(0.1)\n",
    "w_rotation = Value(0.5)\n",
    "params = [w_squeeze, w_rotation]\n",
    "\n",
    "# Data\n",
    "x = 0.5\n",
    "y_target = 1.0\n",
    "\n",
    "# Forward\n",
    "# The 'QuantumOp' acts like a node in the graph. \n",
    "# It takes 'params' as parents and produces a 'pred' value.\n",
    "pred = QuantumOp(my_quantum_layer, params, x)\n",
    "\n",
    "# Loss (MSE)\n",
    "error = pred + Value(-y_target)\n",
    "loss = error * error\n",
    "\n",
    "# Backward\n",
    "# This triggers the _backward_step inside QuantumOp, which\n",
    "# runs the simulator multiple times to shift the parameters.\n",
    "loss.backward()\n",
    "\n",
    "# Update\n",
    "lr = 0.1\n",
    "for p in params:\n",
    "    p.data -= lr * p.grad\n",
    "\n",
    "print(f\"Prediction: {pred.data:.4f}\")\n",
    "print(f\"Loss:       {loss.data:.4f}\")\n",
    "print(f\"Grads:      Sq={w_squeeze.grad:.4f}, Rot={w_rotation.grad:.4f}\")\n",
    "print(f\"New Params: Sq={w_squeeze.data:.4f}, Rot={w_rotation.data:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9135723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "\n",
    "# --- 1. Autograd Engine (Value Class) ---\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = 0\n",
    "\n",
    "# --- 2. Quantum Operation Node ---\n",
    "class QuantumOp(Value):\n",
    "    def __init__(self, circuit_builder, params, input_val):\n",
    "        self.params = params \n",
    "        self.input_val = input_val\n",
    "        self.circuit_builder = circuit_builder\n",
    "        \n",
    "        # Forward Pass\n",
    "        val = self._run_simulation([p.data for p in self.params])\n",
    "        super().__init__(val, tuple(params), 'QuantumGate')\n",
    "\n",
    "    def _run_simulation(self, numeric_params):\n",
    "        # Setup: 2 Modes, 2 Qubits per mode (Cutoff=4)\n",
    "        qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=2)\n",
    "        cr = qiskit.ClassicalRegister(4) \n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "        \n",
    "        self.circuit_builder(circ, numeric_params, self.input_val, qmr)\n",
    "        circ.cv_measure(qmr, cr)\n",
    "        \n",
    "        try:\n",
    "            # Low shots (50) for speed during training\n",
    "            _, res, *_ = bosonic_qiskit.util.simulate(circ, shots=50)\n",
    "            counts = res.get_counts()\n",
    "            \n",
    "            # Metric: Prob of |0,1> (Fraud State)\n",
    "            prob_fraud = 0\n",
    "            for bitstr, count in counts.items():\n",
    "                val = int(bitstr.replace(\" \", \"\"), 2)\n",
    "                # Check for 1 photon in Mode 1 (second mode)\n",
    "                # Mask: last 2 bits are Mode 0, first 2 bits are Mode 1\n",
    "                n_mode_1 = (val >> 2) & 3\n",
    "                if n_mode_1 >= 1: \n",
    "                    prob_fraud += count\n",
    "            \n",
    "            return prob_fraud / 50.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _backward(self):\n",
    "        h = 1e-2 # Finite difference step\n",
    "        \n",
    "        for i, param in enumerate(self.params):\n",
    "            p_vals = [p.data for p in self.params]\n",
    "            \n",
    "            # Shift Right\n",
    "            p_vals[i] += h\n",
    "            out_plus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # Shift Left\n",
    "            p_vals[i] -= 2 * h\n",
    "            out_minus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # Gradient\n",
    "            local_grad = (out_plus - out_minus) / (2 * h)\n",
    "            param.grad += self.grad * local_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b1607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Starting Training on 800 samples...\n",
      "Epoch 1: Processed 800/800 samples...\n",
      "Epoch 1 Completed in 33.8s | Avg Loss: 0.4934\n",
      "Test ROC AUC: 0.2667\n",
      "------------------------------\n",
      "Epoch 2: Processed 800/800 samples...\n",
      "Epoch 2 Completed in 33.3s | Avg Loss: 0.4944\n",
      "Test ROC AUC: 0.4667\n",
      "------------------------------\n",
      "Epoch 3: Processed 800/800 samples...\n",
      "Epoch 3 Completed in 41.6s | Avg Loss: 0.4944\n",
      "Test ROC AUC: 0.6000\n",
      "------------------------------\n",
      "Epoch 4: Processed 800/800 samples...\n",
      "Epoch 4 Completed in 40.5s | Avg Loss: 0.4946\n",
      "Test ROC AUC: 0.4733\n",
      "------------------------------\n",
      "Epoch 5: Processed 800/800 samples...\n",
      "Epoch 5 Completed in 38.1s | Avg Loss: 0.4950\n",
      "Test ROC AUC: 0.5400\n",
      "------------------------------\n",
      "Epoch 6: Processed 800/800 samples...\n",
      "Epoch 6 Completed in 35.8s | Avg Loss: 0.4949\n",
      "Test ROC AUC: 0.3867\n",
      "------------------------------\n",
      "Epoch 7: Processed 800/800 samples...\n",
      "Epoch 7 Completed in 34.4s | Avg Loss: 0.4935\n",
      "Test ROC AUC: 0.4733\n",
      "------------------------------\n",
      "Epoch 8: Processed 800/800 samples...\n",
      "Epoch 8 Completed in 35.8s | Avg Loss: 0.4942\n",
      "Test ROC AUC: 0.4267\n",
      "------------------------------\n",
      "Epoch 9: Processed 800/800 samples...\n",
      "Epoch 9 Completed in 37.7s | Avg Loss: 0.4940\n",
      "Test ROC AUC: 0.4400\n",
      "------------------------------\n",
      "Epoch 10: Processed 800/800 samples...\n",
      "Epoch 10 Completed in 35.2s | Avg Loss: 0.4924\n",
      "Test ROC AUC: 0.3333\n",
      "------------------------------\n",
      "Epoch 11: Processed 800/800 samples...\n",
      "Epoch 11 Completed in 34.7s | Avg Loss: 0.4939\n",
      "Test ROC AUC: 0.2200\n",
      "------------------------------\n",
      "Epoch 12: Processed 800/800 samples...\n",
      "Epoch 12 Completed in 34.3s | Avg Loss: 0.4946\n",
      "Test ROC AUC: 0.2933\n",
      "------------------------------\n",
      "Epoch 13: Processed 800/800 samples...\n",
      "Epoch 13 Completed in 34.5s | Avg Loss: 0.4939\n",
      "Test ROC AUC: 0.2867\n",
      "------------------------------\n",
      "Epoch 14: Processed 800/800 samples...\n",
      "Epoch 14 Completed in 35.8s | Avg Loss: 0.4941\n",
      "Test ROC AUC: 0.4400\n",
      "------------------------------\n",
      "Epoch 15: Processed 800/800 samples...\n",
      "Epoch 15 Completed in 34.6s | Avg Loss: 0.4924\n",
      "Test ROC AUC: 0.3733\n",
      "------------------------------\n",
      "Final Weights:\n",
      "[0.01, 0.05, 0.1, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "# --- A. Data Loading (Same as before) ---\n",
    "def get_data_for_training(filepath=r\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\", n_samples=200):\n",
    "    # Load and balance\n",
    "    df = pd.read_csv(filepath)\n",
    "    fraud = df[df['Class'] == 1]\n",
    "    normal = df[df['Class'] == 0]\n",
    "    \n",
    "    n_per = n_samples // 2\n",
    "    fraud = resample(fraud, n_samples=n_per, random_state=42)\n",
    "    normal = resample(normal, n_samples=n_per, random_state=42)\n",
    "    df_balanced = pd.concat([fraud, normal])\n",
    "    \n",
    "    X = df_balanced.drop(['Class', 'Time'], axis=1)\n",
    "    y = df_balanced['Class'].values\n",
    "    \n",
    "    # Scale & PCA\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Reduce to just 1 feature for this demo (Speed!)\n",
    "    # Or use 2 features if you have patience.\n",
    "    pca = PCA(n_components=2) \n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- B. Define Quantum Circuit ---\n",
    "def fraud_circuit(circuit, params, x_input, qmr):\n",
    "    \"\"\"\n",
    "    params[0]: Squeezing\n",
    "    params[1]: Rotation\n",
    "    params[2]: Beam Splitter\n",
    "    params[3]: Displacement bias\n",
    "    \"\"\"\n",
    "    # Encoder: Data -> Displacement on Mode 0\n",
    "    # x_input is an array/list from PCA\n",
    "    val = x_input[0] \n",
    "    circuit.cv_d(val, qmr[0])\n",
    "    \n",
    "    # Trainable Variational Layer\n",
    "    circuit.cv_sq(params[0], qmr[0])\n",
    "    circuit.cv_r(params[1], qmr[0])\n",
    "    \n",
    "    # Coupling to Mode 1 (where we measure fraud)\n",
    "    circuit.cv_bs(params[2], qmr[0], qmr[1])\n",
    "    \n",
    "    # Bias displacement\n",
    "    circuit.cv_d(params[3], qmr[1])\n",
    "\n",
    "# --- C. Training Loop ---\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"Loading Data...\")\n",
    "# Adjust 'n_samples' based on your patience. 100 samples ~ 5 mins training.\n",
    "X_train, X_test, y_train, y_test = get_data_for_training(filepath=\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\", n_samples=1000)\n",
    "\n",
    "# 2. Init Weights (Value objects)\n",
    "w_sq = Value(0.01)\n",
    "w_rot = Value(0.05)\n",
    "w_bs = Value(0.1)\n",
    "w_bias = Value(0.0)\n",
    "params = [w_sq, w_rot, w_bs, w_bias]\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 15\n",
    "batch_size = 20\n",
    "learning_rate = 0.05\n",
    "\n",
    "print(f\"Starting Training on {len(X_train)} samples...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Mini-Batch Loop\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i : i + batch_size]\n",
    "        batch_loss = Value(0) # Accumulator for batch loss\n",
    "        \n",
    "        # 1. Forward Pass (Accumulate Loss)\n",
    "        for idx in batch_idx:\n",
    "            x_in = X_train[idx]\n",
    "            y_target = y_train[idx] # 0 or 1\n",
    "            \n",
    "            # Build Graph\n",
    "            pred = QuantumOp(fraud_circuit, params, x_in)\n",
    "            \n",
    "            # MSE Loss: (pred - target)^2\n",
    "            diff = pred + Value(-float(y_target))\n",
    "            sample_loss = diff * diff\n",
    "            \n",
    "            batch_loss = batch_loss + sample_loss\n",
    "        \n",
    "        # Average the loss\n",
    "        batch_loss = batch_loss * Value(1.0 / batch_size)\n",
    "        epoch_loss += batch_loss.data\n",
    "        \n",
    "        # 2. Zero Gradients\n",
    "        for p in params: p.zero_grad()\n",
    "            \n",
    "        # 3. Backward Pass\n",
    "        # This triggers simulations for the whole batch\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # 4. Update\n",
    "        for p in params:\n",
    "            p.data -= learning_rate * p.grad\n",
    "            \n",
    "        print(f\"\\rEpoch {epoch+1}: Processed {i+len(batch_idx)}/{len(X_train)} samples...\", end=\"\")\n",
    "    \n",
    "    # Validation at end of epoch\n",
    "    print(f\"\\nEpoch {epoch+1} Completed in {time.time()-start_time:.1f}s | Avg Loss: {epoch_loss / (len(X_train)/batch_size):.4f}\")\n",
    "    \n",
    "    # Check AUC on Test Set (subset)\n",
    "    test_preds = []\n",
    "    for x in X_test[:20]:\n",
    "        # Fast inference (no gradients needed)\n",
    "        # We can just use the forward pass logic of QuantumOp manually or create nodes\n",
    "        p = QuantumOp(fraud_circuit, params, x).data\n",
    "        test_preds.append(p)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test[:20], test_preds)\n",
    "        print(f\"Test ROC AUC: {auc:.4f}\")\n",
    "    except:\n",
    "        print(\"AUC Calc Error (likely single class in batch)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"Final Weights:\")\n",
    "print([p.data for p in params])"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 69bbff89a3ac5f98702924a2e347b009be7eb41a
   "execution_count": null,
   "id": "f7e0400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with BINARY CROSS-ENTROPY loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\Dual Degree Project\\DDP\\boson_env\\lib\\site-packages\\scipy\\sparse\\_index.py:168: SparseEfficiencyWarning: Changing the sparsity structure of a csc_array is expensive. lil and dok are more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56 | Loss: 1.3554Simulation failed: 'HighLevelSynthesis is unable to synthesize \"BS\"'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cmath\n",
    "\n",
    "# --- 1. Autograd Engine (Value Class) ---\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * Value(-1)\n",
    "\n",
    "    def log(self):\n",
    "        val = self.data\n",
    "        # Clip to prevent log(0) = -inf\n",
    "        if val < 1e-7: val = 1e-7\n",
    "        if val > 1.0 - 1e-7: val = 1.0 - 1e-7\n",
    "        \n",
    "        out = Value(np.log(val), (self,), 'log')\n",
    "        def _backward():\n",
    "            self.grad += (1.0 / val) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def tanh(self):\n",
    "        # Tanh activation allows negative values (Phase Space!)\n",
    "        x = self.data\n",
    "        t = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        self.grad = 0\n",
    "\n",
    "# --- 2. Improved Quantum Op (Using a Custom Observable) ---\n",
    "class QuantumOp(Value):\n",
<<<<<<< HEAD
    "    def __init__(self, circuit_builder, weights, inputs, cutoff_dim=2):\n",
=======
    "    def __init__(self, circuit_builder, weights, inputs, cutoff_dim=4):\n",
>>>>>>> 69bbff89a3ac5f98702924a2e347b009be7eb41a
    "        self.weights = weights\n",
    "        self.inputs = inputs\n",
    "        self.circuit_builder = circuit_builder\n",
    "        self.cutoff_dim = cutoff_dim\n",
    "        \n",
    "        # --- Define the observable for our HYBRID classification task ---\n",
    "        cv_dim = self.cutoff_dim ** 2\n",
    "        \n",
    "        o_cv = np.zeros((cv_dim, cv_dim))\n",
    "        fraud_idx = 1\n",
    "        normal_idx = self.cutoff_dim\n",
    "        o_cv[fraud_idx, fraud_idx] = 1.0\n",
    "        o_cv[normal_idx, normal_idx] = -1.0\n",
    "        \n",
    "        i_qubit = np.identity(2)\n",
    "        self.observable = np.kron(i_qubit, o_cv)\n",
    "        \n",
    "        parents = tuple(weights + inputs)\n",
    "        \n",
    "        w_vals = [w.data for w in self.weights]\n",
    "        i_vals = [i.data for i in self.inputs]\n",
    "        val = self._run_simulation(w_vals, i_vals)\n",
    "        super().__init__(val, parents, 'QuantumHybrid')\n",
    "\n",
    "    def _run_simulation(self, w_vals, i_vals):\n",
    "        # 1. Setup a HYBRID Circuit\n",
    "        qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=self.cutoff_dim)\n",
    "        qr = qiskit.QuantumRegister(1)\n",
    "        cr = qiskit.ClassicalRegister(3)\n",
    "        \n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, qr, cr)\n",
    "        \n",
    "        # 2. Build User Circuit, passing both registers\n",
    "        self.circuit_builder(circ, w_vals, i_vals, qmr, qr)\n",
    "        \n",
    "        try:\n",
    "            state, _, _ = bosonic_qiskit.util.simulate(circ)\n",
    "            probs = state.probabilities()\n",
    "            p_fraud = probs[1]\n",
    "            return p_fraud\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed: {e}\")\n",
    "            return 0.5\n",
    "\n",
    "    def _backward(self):\n",
    "        h = 0.001 \n",
    "        w_numerics = [w.data for w in self.weights]\n",
    "        i_numerics = [i.data for i in self.inputs]\n",
    "        for idx, w in enumerate(self.weights):\n",
    "            w_copy = w_numerics.copy()\n",
    "            w_copy[idx] += h\n",
    "            out_plus = self._run_simulation(w_copy, i_numerics)\n",
    "            w_copy[idx] -= 2*h\n",
    "            out_minus = self._run_simulation(w_copy, i_numerics)\n",
    "            grad = (out_plus - out_minus) / (2*h)\n",
    "            w.grad += self.grad * grad\n",
    "        for idx, inp in enumerate(self.inputs):\n",
    "            i_copy = i_numerics.copy()\n",
    "            i_copy[idx] += h\n",
    "            out_plus = self._run_simulation(w_numerics, i_copy)\n",
    "            i_copy[idx] -= 2*h\n",
    "            out_minus = self._run_simulation(w_numerics, i_copy)\n",
    "            grad = (out_plus - out_minus) / (2*h)\n",
    "            inp.grad += self.grad * grad\n",
    "\n",
    "# --- 3. Hybrid Model with Tanh & Strong Init ---\n",
    "class LinearLayer:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.weights = [[Value(random.gauss(0, 0.2)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "        self.bias = [Value(0.1) for _ in range(n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for j in range(len(self.bias)):\n",
    "            act = self.bias[j]\n",
    "            for i in range(len(x)):\n",
    "                act = act + x[i] * self.weights[i][j]\n",
    "            out.append(act)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for row in self.weights for p in row] + self.bias\n",
    "\n",
    "class HybridFraudDetector:\n",
    "    def __init__(self, n_features):\n",
    "        self.c_layer1 = LinearLayer(n_features, 20)\n",
    "        self.c_layer2 = LinearLayer(20, 14) \n",
    "        \n",
    "        self.q_weights = []\n",
    "        for i in range(21):\n",
    "            val = random.uniform(-0.1, 0.1) \n",
    "            self.q_weights.append(Value(val))\n",
    "            \n",
    "    def forward(self, x_raw):\n",
    "        x_vals = [Value(xi) for xi in x_raw]\n",
    "        h1 = self.c_layer1(x_vals)\n",
    "        h1_act = [h.tanh() for h in h1]\n",
    "        encoding_params = self.c_layer2(h1_act) \n",
    "        out = QuantumOp(self.quantum_circuit, self.q_weights, encoding_params)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantum_circuit(circuit, weights, inputs, qmr, qr):\n",
    "        def to_complex(mag, phase):\n",
    "            return mag * cmath.exp(1j * phase)\n",
    "\n",
    "        sq_0 = to_complex(inputs[0], inputs[1])\n",
    "        sq_1 = to_complex(inputs[2], inputs[3])\n",
    "        circuit.cv_sq(sq_0, qmr[0])\n",
    "        circuit.cv_sq(sq_1, qmr[1])\n",
    "        circuit.cv_bs(inputs[4], qmr[0], qmr[1])\n",
    "        circuit.cv_r(inputs[6], qmr[0])\n",
    "        circuit.cv_r(inputs[7], qmr[1])\n",
    "        d_0 = to_complex(inputs[8], inputs[9])\n",
    "        d_1 = to_complex(inputs[10], inputs[11])\n",
    "        circuit.cv_d(d_0, qmr[0])\n",
    "        circuit.cv_d(d_1, qmr[1])\n",
    "        \n",
    "        circuit.cv_bs(weights[0], qmr[0], qmr[1])\n",
    "        circuit.cv_r(weights[2], qmr[0])\n",
    "        circuit.cv_r(weights[3], qmr[1])\n",
    "        s_var_0 = to_complex(weights[4], weights[5])\n",
    "        s_var_1 = to_complex(weights[6], weights[7])\n",
    "        circuit.cv_sq(s_var_0, qmr[0])\n",
    "        circuit.cv_sq(s_var_1, qmr[1])\n",
    "        circuit.cv_bs(weights[8], qmr[0], qmr[1])\n",
    "        circuit.cv_r(weights[10], qmr[0])\n",
    "        circuit.cv_r(weights[11], qmr[1])\n",
    "        d_var_0 = to_complex(weights[12], weights[13])\n",
    "        d_var_1 = to_complex(weights[14], weights[15])\n",
    "        circuit.cv_d(d_var_0, qmr[0])\n",
    "        circuit.cv_d(d_var_1, qmr[1])\n",
    "        \n",
    "        circuit.h(qr[0])\n",
    "        circuit.ry(weights[18], qr[0])\n",
    "        \n",
    "        cd_alpha = to_complex(weights[19], weights[20])\n",
    "        circuit.cv_c_d(cd_alpha, [qmr[0]], qr[0])\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.c_layer1.parameters() + self.c_layer2.parameters() + self.q_weights\n",
    "    \n",
    "# --- 4. Training - MODIFIED FOR STABILITY ---\n",
    "def get_full_data(n_samples=1000):\n",
    "    try:\n",
    "        df = pd.read_csv(r\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: Please update the path to 'creditcard_data.csv'\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    fraud = df[df['Class'] == 1]\n",
    "    normal = df[df['Class'] == 0]\n",
    "    n_per = n_samples // 2\n",
    "    fraud = resample(fraud, n_samples=n_per, random_state=42)\n",
    "    normal = resample(normal, n_samples=n_per, random_state=42)\n",
    "    df_bal = pd.concat([fraud, normal])\n",
    "    \n",
    "    X = df_bal.drop(['Class', 'Time'], axis=1).values\n",
    "    y = df_bal['Class'].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup\n",
    "X_train, X_test, y_train, y_test = get_full_data(n_samples=10000)\n",
    "if X_train is not None:\n",
    "    model = HybridFraudDetector(n_features=29)\n",
    "    optimizer_params = model.parameters()\n",
    "    epochs = 10\n",
    "    \n",
    "    # --- CHANGE 1: Lower the learning rate ---\n",
    "    # Cross-entropy can produce large gradients, so a smaller LR is needed for stability.\n",
    "    lr = 0.001 \n",
    "    \n",
    "    print(f\"Training with BINARY CROSS-ENTROPY loss...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        start = time.time()\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        \n",
    "        for i in range(0, len(X_train), 64):\n",
    "            batch_idx = indices[i:i+64]\n",
    "            batch_loss = Value(0)\n",
    "            \n",
    "            for idx in batch_idx:\n",
    "                # pred is now a probability p in [0, 1]\n",
    "                p = model.forward(X_train[idx])\n",
    "                y_target = y_train[idx]\n",
    "                \n",
    "                # --- CHANGE FOR CROSS-ENTROPY ---\n",
    "                # Loss = -(y*log(p) + (1-y)*log(1-p))\n",
    "                y = Value(y_target)\n",
    "                term = (y * p.log() + (Value(1) + y.__neg__()) * (Value(1) + p.__neg__()).log()).__neg__()\n",
    "                \n",
    "                batch_loss = batch_loss + term\n",
    "                \n",
    "            batch_loss = batch_loss * Value(1.0/64)\n",
    "            \n",
    "            # Zero Grads & Backprop\n",
    "            for p in optimizer_params: p.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            # --- CHANGE 2: Implement Gradient Clipping ---\n",
    "            # This prevents exploding gradients from destabilizing the training.\n",
    "            clip_value = 1.0 # Do not allow any single gradient to be larger than this.\n",
    "            for p in optimizer_params:\n",
    "                p.grad = np.clip(p.grad, -clip_value, clip_value)\n",
    "\n",
    "            # Update weights\n",
    "            for p in optimizer_params:\n",
    "                p.data -= lr * p.grad\n",
    "                \n",
    "            epoch_loss += batch_loss.data\n",
    "            print(f\"\\rBatch {i//64} | Loss: {batch_loss.data:.4f}\", end=\"\")\n",
    "            \n",
    "        print(f\"\\nEpoch {epoch+1} | Avg Loss: {epoch_loss / (len(X_train)/64):.4f} | Time: {time.time()-start:.1f}s\")\n",
    "        \n",
    "        # Test AUC (still works with probability outputs)\n",
    "        preds = [model.forward(x).data for x in X_test]\n",
    "        try:\n",
    "            print(f\"Test AUC: {roc_auc_score(y_test, preds):.4f}\")\n",
    "        except: pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boson_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
