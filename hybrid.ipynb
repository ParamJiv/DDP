{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168e220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Manual Gradients on 43 parameters.\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 1: Loss=0.100 | AUC=0.500\n",
      "  >>> New Best AUC! Model Saved.\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 2: Loss=0.100 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 3: Loss=0.200 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 4: Loss=0.100 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 5: Loss=0.100 | AUC=0.500\n",
      "\n",
      "Final Evaluation on Test Set...\n",
      "Final Test ROC AUC: 0.6250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, roc_curve\n",
    ")\n",
    "\n",
    "# System and Warning Handling\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA GENERATION (Simulated Fraud Data)\n",
    "# ==========================================\n",
    "def get_data():\n",
    "    # Reduced features to 4 for speed (Manual Gradients are slow!)\n",
    "    X, y = make_classification(n_samples=300, n_features=4, n_informative=4, \n",
    "                               n_redundant=0, weights=[0.9, 0.1], random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data()\n",
    "\n",
    "# ==========================================\n",
    "# 2. HYBRID MODEL (Forward Pass)\n",
    "# ==========================================\n",
    "def apply_kerr(circuit, kerr_param, qumode, cutoff):\n",
    "    \"\"\"Manual Kerr Gate using SNAP\"\"\"\n",
    "    for n in range(cutoff):\n",
    "        phase = kerr_param * (n ** 2)\n",
    "        if abs(phase) > 1e-4:\n",
    "            circuit.cv_snap(phase, n, qumode, None)\n",
    "\n",
    "def get_probs(result):\n",
    "    \"\"\"Extract probabilities for Class 0 (|1,0>) and Class 1 (|0,1>)\"\"\"\n",
    "    counts = result.get_counts()\n",
    "    total_shots = sum(counts.values())\n",
    "    p_10, p_01 = 0, 0\n",
    "    \n",
    "    for bitstr, count in counts.items():\n",
    "        val = int(bitstr.replace(\" \", \"\"), 2)\n",
    "        # Assuming 2 qubits per mode (Cutoff=4)\n",
    "        n_a = val & 3\n",
    "        n_b = (val >> 2) & 3\n",
    "        \n",
    "        if n_a == 1 and n_b == 0: p_10 += count\n",
    "        elif n_a == 0 and n_b == 1: p_01 += count\n",
    "            \n",
    "    return (p_10/total_shots, p_01/total_shots) if total_shots > 0 else (0,0)\n",
    "\n",
    "def hybrid_forward(params, x_input):\n",
    "    # --- Classical NN (4 inputs -> 8 outputs) ---\n",
    "    # Reduced size for speed\n",
    "    n_in, n_out = 4, 8\n",
    "    n_classical = n_in * n_out + n_out\n",
    "    \n",
    "    w = params[:n_in*n_out].reshape(n_in, n_out)\n",
    "    b = params[n_in*n_out : n_classical]\n",
    "    \n",
    "    # NN Output used as parameters for the encoding layer\n",
    "    nn_out = np.tanh(np.dot(x_input, w) + b)\n",
    "    \n",
    "    # --- Quantum Circuit ---\n",
    "    qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=2)\n",
    "    cr = qiskit.ClassicalRegister(4)\n",
    "    circuit = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "    \n",
    "    # Encoding (Driven by NN output)\n",
    "    # Using first 8 params for: Sq(0), Sq(1), BS, R(0), R(1), D(0), D(1), Kerr(0)\n",
    "    circuit.cv_sq(nn_out[0], qmr[0])\n",
    "    circuit.cv_sq(nn_out[1], qmr[1])\n",
    "    circuit.cv_bs(nn_out[2], qmr[0], qmr[1])\n",
    "    circuit.cv_r(nn_out[3], qmr[0])\n",
    "    circuit.cv_d(nn_out[4], qmr[0])\n",
    "    apply_kerr(circuit, nn_out[5], qmr[0], 4)\n",
    "    \n",
    "    # Variational Layer (Trainable Quantum Params)\n",
    "    q_params = params[n_classical:]\n",
    "    circuit.cv_bs(q_params[0], qmr[0], qmr[1])\n",
    "    circuit.cv_r(q_params[1], qmr[0])\n",
    "    circuit.cv_snap(q_params[2], 1, qmr[0], None) # Non-linearity\n",
    "\n",
    "    circuit.cv_measure(qmr, cr)\n",
    "    \n",
    "    # Simulate\n",
    "    try:\n",
    "        _, res, *_ = bosonic_qiskit.util.simulate(circuit, shots=100)\n",
    "        return get_probs(res)\n",
    "    except:\n",
    "        return 0.5, 0.5\n",
    "\n",
    "# ==========================================\n",
    "# 3. MANUAL GRADIENT CALCULATION\n",
    "# ==========================================\n",
    "def get_batch_loss(params, X_batch, y_batch):\n",
    "    loss = 0\n",
    "    scores = []\n",
    "    \n",
    "    for x, y in zip(X_batch, y_batch):\n",
    "        p_10, p_01 = hybrid_forward(params, x)\n",
    "        \n",
    "        # Loss: MSE distance from ideal probability\n",
    "        # Class 0 -> Target p_10=1.0 (p_01=0.0)\n",
    "        # Class 1 -> Target p_01=1.0 \n",
    "        target = 1.0 if y == 1 else 0.0\n",
    "        pred = p_01\n",
    "        \n",
    "        loss += (pred - target) ** 2\n",
    "        scores.append(pred) # Score for AUC is p_01 (Fraud Probability)\n",
    "        \n",
    "    return loss / len(y_batch), scores\n",
    "\n",
    "def compute_gradients_finite_diff(params, X_batch, y_batch, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Manually calculates gradient for EACH parameter.\n",
    "    Run time = 2 * N_params * Batch_Size * Simulator_Time\n",
    "    \"\"\"\n",
    "    grads = np.zeros_like(params)\n",
    "    \n",
    "    # Base loss (optional, for debugging)\n",
    "    # base_loss, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "    \n",
    "    print(f\"  Computing Gradients for {len(params)} parameters...\", end=\"\", flush=True)\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        # Shift Right\n",
    "        params[i] += epsilon\n",
    "        loss_plus, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "        \n",
    "        # Shift Left\n",
    "        params[i] -= 2 * epsilon # Move back to -epsilon\n",
    "        loss_minus, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "        \n",
    "        # Reset\n",
    "        params[i] += epsilon \n",
    "        \n",
    "        # Central Difference\n",
    "        grads[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        if i % 10 == 0: print(\".\", end=\"\", flush=True)\n",
    "            \n",
    "    print(\" Done.\")\n",
    "    return grads\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP (MAXIMIZE AUC)\n",
    "# ==========================================\n",
    "# Setup Parameters: 4*8 (Weights) + 8 (Bias) + 3 (Quantum) = 43 params\n",
    "n_params = (4 * 8) + 8 + 3\n",
    "weights = np.random.normal(0, 0.1, n_params)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 5 # Small number due to simulation time\n",
    "batch_size = 4 \n",
    "\n",
    "best_val_auc = 0\n",
    "best_weights = None\n",
    "\n",
    "print(f\"Starting Training with Manual Gradients on {n_params} parameters.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Mini-Batch Training\n",
    "    indices = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "    X_batch = X_train[indices]\n",
    "    y_batch = y_train[indices]\n",
    "    \n",
    "    # 2. Compute Manual Gradients\n",
    "    grads = compute_gradients_finite_diff(weights, X_batch, y_batch)\n",
    "    \n",
    "    # 3. Update Weights (Gradient Descent)\n",
    "    weights = weights - (learning_rate * grads)\n",
    "    \n",
    "    # 4. Validation & AUC Check\n",
    "    # We evaluate on a hold-out set to find the Best AUC model\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    val_indices = np.random.choice(len(X_test), 10, replace=False) # Subset for speed\n",
    "    \n",
    "    for idx in val_indices:\n",
    "        p_10, p_01 = hybrid_forward(weights, X_test[idx])\n",
    "        val_preds.append(p_01)\n",
    "        val_loss += (p_01 - (1 if y_test[idx]==1 else 0))**2\n",
    "        \n",
    "    try:\n",
    "        # Calculate AUC (needs both classes present in the batch)\n",
    "        if len(set(y_test[val_indices])) > 1:\n",
    "            current_auc = roc_auc_score(y_test[val_indices], val_preds)\n",
    "        else:\n",
    "            current_auc = 0.5\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={val_loss/10:.3f} | AUC={current_auc:.3f}\")\n",
    "        \n",
    "        # SAVE THE MODEL WITH BEST AUC\n",
    "        if current_auc > best_val_auc:\n",
    "            best_val_auc = current_auc\n",
    "            best_weights = weights.copy()\n",
    "            print(\"  >>> New Best AUC! Model Saved.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Metric Calc Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FINAL TEST\n",
    "# ==========================================\n",
    "print(\"\\nFinal Evaluation on Test Set...\")\n",
    "final_preds = []\n",
    "for x in X_test[:30]: # Limit for speed\n",
    "    _, p_01 = hybrid_forward(best_weights if best_weights is not None else weights, x)\n",
    "    final_preds.append(p_01)\n",
    "\n",
    "final_auc = roc_auc_score(y_test[:30], final_preds)\n",
    "print(f\"Final Test ROC AUC: {final_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with MEAN PHOTON metric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\Dual Degree Project\\DDP\\boson_env\\lib\\site-packages\\scipy\\sparse\\_index.py:168: SparseEfficiencyWarning: Changing the sparsity structure of a csc_array is expensive. lil and dok are more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 124 | Loss: 1.0671\n",
      "Epoch 1 | Avg Loss: 1.0822 | Time: 839.5s\n",
      "Test AUC: 0.3764\n",
      "Batch 1 | Loss: 0.9781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\Dual Degree Project\\DDP\\boson_env\\lib\\site-packages\\scipy\\sparse\\_index.py:168: SparseEfficiencyWarning: Changing the sparsity structure of a csc_array is expensive. lil and dok are more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 | Loss: 1.0767"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 332\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Inside your training loop...\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch_idx:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;66;03m# pred range is [-1.0, +1.0]\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m     y_target \u001b[38;5;241m=\u001b[39m y_train[idx]\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# --- Robust MSE (Fidelity Loss) ---\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 209\u001b[0m, in \u001b[0;36mHybridFraudDetector.forward\u001b[1;34m(self, x_raw)\u001b[0m\n\u001b[0;32m    206\u001b[0m encoding_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_layer2(h1_act) \n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Quantum Pass\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mQuantumOp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantum_circuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m, in \u001b[0;36mQuantumOp.__init__\u001b[1;34m(self, circuit_builder, weights, inputs)\u001b[0m\n\u001b[0;32m     91\u001b[0m w_vals \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[0;32m     92\u001b[0m i_vals \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs]\n\u001b[1;32m---> 93\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(val, parents, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantumHybrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 103\u001b[0m, in \u001b[0;36mQuantumOp._run_simulation\u001b[1;34m(self, w_vals, i_vals)\u001b[0m\n\u001b[0;32m    100\u001b[0m circ \u001b[38;5;241m=\u001b[39m bosonic_qiskit\u001b[38;5;241m.\u001b[39mCVCircuit(qmr, cr)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 2. Build User Circuit\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcirc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# 3. CRITICAL CHANGE: Do NOT add cv_measure here.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# We want the state vector, not the measurement counts.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[1], line 240\u001b[0m, in \u001b[0;36mHybridFraudDetector.quantum_circuit\u001b[1;34m(circuit, weights, inputs, qmr)\u001b[0m\n\u001b[0;32m    236\u001b[0m circuit\u001b[38;5;241m.\u001b[39mcv_sq(sq_1, qmr[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Beam Splitter: BSgate(layer[:,4], layer[:,5])\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Bosonic Qiskit BS takes (theta, phi, q1, q2)\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_bs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqmr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqmr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Rotation: Rgate(layer[:,6])\u001b[39;00m\n\u001b[0;32m    243\u001b[0m circuit\u001b[38;5;241m.\u001b[39mcv_r(inputs[\u001b[38;5;241m6\u001b[39m], qmr[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32md:\\Academic\\Dual Degree Project\\DDP\\boson_env\\lib\\site-packages\\bosonic_qiskit\\circuit.py:477\u001b[0m, in \u001b[0;36mCVCircuit.cv_bs\u001b[1;34m(self, theta, qumode_a, qumode_b, duration, unit)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcv_bs\u001b[39m(\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    456\u001b[0m     theta: \u001b[38;5;28mcomplex\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m     unit: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    461\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InstructionSet:\n\u001b[0;32m    462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Two-mode beam splitter gate.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m        Instruction: QisKit instruction\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_gate(\n\u001b[0;32m    474\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbs,\n\u001b[0;32m    475\u001b[0m             [theta],\n\u001b[0;32m    476\u001b[0m             cutoffs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m--> 477\u001b[0m                 \u001b[43mQumodeRegister\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_cutoff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqumode_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    478\u001b[0m                 QumodeRegister\u001b[38;5;241m.\u001b[39mcalculate_cutoff(\u001b[38;5;28mlen\u001b[39m(qumode_b)),\n\u001b[0;32m    479\u001b[0m             ],\n\u001b[0;32m    480\u001b[0m             num_qubits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(qumode_a) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(qumode_b),\n\u001b[0;32m    481\u001b[0m             label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    482\u001b[0m             duration\u001b[38;5;241m=\u001b[39mduration,\n\u001b[0;32m    483\u001b[0m             unit\u001b[38;5;241m=\u001b[39munit,\n\u001b[0;32m    484\u001b[0m         ),\n\u001b[0;32m    485\u001b[0m         qargs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m*\u001b[39mqumode_a, \u001b[38;5;241m*\u001b[39mqumode_b],\n\u001b[0;32m    486\u001b[0m     )\n",
      "File \u001b[1;32md:\\Academic\\Dual Degree Project\\DDP\\boson_env\\lib\\site-packages\\bosonic_qiskit\\qumoderegister.py:47\u001b[0m, in \u001b[0;36mQumodeRegister.calculate_cutoff\u001b[1;34m(num_qubits_per_qumode)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcutoff\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_cutoff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_qubits_per_qumode)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculate_cutoff\u001b[39m(num_qubits_per_qumode: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnum_qubits_per_qumode\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_qumode_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, qubit: Qubit) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cmath\n",
    "\n",
    "# --- 1. Autograd Engine (Value Class) ---\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * Value(-1)\n",
    "\n",
    "    def log(self):\n",
    "        val = self.data\n",
    "        # Clip to prevent log(0) = -inf\n",
    "        if val < 1e-7: val = 1e-7\n",
    "        if val > 1.0 - 1e-7: val = 1.0 - 1e-7\n",
    "        \n",
    "        out = Value(np.log(val), (self,), 'log')\n",
    "        def _backward():\n",
    "            self.grad += (1.0 / val) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def tanh(self):\n",
    "        # Tanh activation allows negative values (Phase Space!)\n",
    "        x = self.data\n",
    "        t = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        self.grad = 0\n",
    "\n",
    "# --- 2. Improved Quantum Op (Using a Custom Observable) ---\n",
    "class QuantumOp(Value):\n",
    "    def __init__(self, circuit_builder, weights, inputs, cutoff_dim=2):\n",
    "        self.weights = weights\n",
    "        self.inputs = inputs\n",
    "        self.circuit_builder = circuit_builder\n",
    "        self.cutoff_dim = cutoff_dim\n",
    "        \n",
    "        # --- Define the observable for our HYBRID classification task ---\n",
    "        cv_dim = self.cutoff_dim ** 2\n",
    "        \n",
    "        o_cv = np.zeros((cv_dim, cv_dim))\n",
    "        fraud_idx = 1\n",
    "        normal_idx = self.cutoff_dim\n",
    "        o_cv[fraud_idx, fraud_idx] = 1.0\n",
    "        o_cv[normal_idx, normal_idx] = -1.0\n",
    "        \n",
    "        i_qubit = np.identity(2)\n",
    "        self.observable = np.kron(i_qubit, o_cv)\n",
    "        \n",
    "        parents = tuple(weights + inputs)\n",
    "        \n",
    "        w_vals = [w.data for w in self.weights]\n",
    "        i_vals = [i.data for i in self.inputs]\n",
    "        val = self._run_simulation(w_vals, i_vals)\n",
    "        super().__init__(val, parents, 'QuantumHybrid')\n",
    "\n",
    "    def _run_simulation(self, w_vals, i_vals):\n",
    "        # 1. Setup a HYBRID Circuit\n",
    "        qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=self.cutoff_dim)\n",
    "        qr = qiskit.QuantumRegister(1)\n",
    "        cr = qiskit.ClassicalRegister(3)\n",
    "        \n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, qr, cr)\n",
    "        \n",
    "        # 2. Build User Circuit, passing both registers\n",
    "        self.circuit_builder(circ, w_vals, i_vals, qmr, qr)\n",
    "        \n",
    "        try:\n",
    "            state, _, _ = bosonic_qiskit.util.simulate(circ)\n",
    "            probs = state.probabilities()\n",
    "            p_fraud = probs[1]\n",
    "            return p_fraud\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed: {e}\")\n",
    "            return 0.5\n",
    "\n",
    "    def _backward(self):\n",
    "        h = 0.001 \n",
    "        w_numerics = [w.data for w in self.weights]\n",
    "        i_numerics = [i.data for i in self.inputs]\n",
    "        for idx, w in enumerate(self.weights):\n",
    "            w_copy = w_numerics.copy()\n",
    "            w_copy[idx] += h\n",
    "            out_plus = self._run_simulation(w_copy, i_numerics)\n",
    "            w_copy[idx] -= 2*h\n",
    "            out_minus = self._run_simulation(w_copy, i_numerics)\n",
    "            grad = (out_plus - out_minus) / (2*h)\n",
    "            w.grad += self.grad * grad\n",
    "        for idx, inp in enumerate(self.inputs):\n",
    "            i_copy = i_numerics.copy()\n",
    "            i_copy[idx] += h\n",
    "            out_plus = self._run_simulation(w_numerics, i_copy)\n",
    "            i_copy[idx] -= 2*h\n",
    "            out_minus = self._run_simulation(w_numerics, i_copy)\n",
    "            grad = (out_plus - out_minus) / (2*h)\n",
    "            inp.grad += self.grad * grad\n",
    "\n",
    "# --- 3. Hybrid Model with Tanh & Strong Init ---\n",
    "class LinearLayer:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.weights = [[Value(random.gauss(0, 0.2)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "        self.bias = [Value(0.1) for _ in range(n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for j in range(len(self.bias)):\n",
    "            act = self.bias[j]\n",
    "            for i in range(len(x)):\n",
    "                act = act + x[i] * self.weights[i][j]\n",
    "            out.append(act)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for row in self.weights for p in row] + self.bias\n",
    "\n",
    "class HybridFraudDetector:\n",
    "    def __init__(self, n_features):\n",
    "        self.c_layer1 = LinearLayer(n_features, 20)\n",
    "        self.c_layer2 = LinearLayer(20, 14) \n",
    "        \n",
    "        self.q_weights = []\n",
    "        for i in range(21):\n",
    "            val = random.uniform(-0.1, 0.1) \n",
    "            self.q_weights.append(Value(val))\n",
    "            \n",
    "    def forward(self, x_raw):\n",
    "        x_vals = [Value(xi) for xi in x_raw]\n",
    "        h1 = self.c_layer1(x_vals)\n",
    "        h1_act = [h.tanh() for h in h1]\n",
    "        encoding_params = self.c_layer2(h1_act) \n",
    "        out = QuantumOp(self.quantum_circuit, self.q_weights, encoding_params)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantum_circuit(circuit, weights, inputs, qmr, qr):\n",
    "        def to_complex(mag, phase):\n",
    "            return mag * cmath.exp(1j * phase)\n",
    "\n",
    "        sq_0 = to_complex(inputs[0], inputs[1])\n",
    "        sq_1 = to_complex(inputs[2], inputs[3])\n",
    "        circuit.cv_sq(sq_0, qmr[0])\n",
    "        circuit.cv_sq(sq_1, qmr[1])\n",
    "        circuit.cv_bs(inputs[4], qmr[0], qmr[1])\n",
    "        circuit.cv_r(inputs[6], qmr[0])\n",
    "        circuit.cv_r(inputs[7], qmr[1])\n",
    "        d_0 = to_complex(inputs[8], inputs[9])\n",
    "        d_1 = to_complex(inputs[10], inputs[11])\n",
    "        circuit.cv_d(d_0, qmr[0])\n",
    "        circuit.cv_d(d_1, qmr[1])\n",
    "        \n",
    "        circuit.cv_bs(weights[0], qmr[0], qmr[1])\n",
    "        circuit.cv_r(weights[2], qmr[0])\n",
    "        circuit.cv_r(weights[3], qmr[1])\n",
    "        s_var_0 = to_complex(weights[4], weights[5])\n",
    "        s_var_1 = to_complex(weights[6], weights[7])\n",
    "        circuit.cv_sq(s_var_0, qmr[0])\n",
    "        circuit.cv_sq(s_var_1, qmr[1])\n",
    "        circuit.cv_bs(weights[8], qmr[0], qmr[1])\n",
    "        circuit.cv_r(weights[10], qmr[0])\n",
    "        circuit.cv_r(weights[11], qmr[1])\n",
    "        d_var_0 = to_complex(weights[12], weights[13])\n",
    "        d_var_1 = to_complex(weights[14], weights[15])\n",
    "        circuit.cv_d(d_var_0, qmr[0])\n",
    "        circuit.cv_d(d_var_1, qmr[1])\n",
    "        \n",
    "        circuit.h(qr[0])\n",
    "        circuit.ry(weights[18], qr[0])\n",
    "        \n",
    "        cd_alpha = to_complex(weights[19], weights[20])\n",
    "        circuit.cv_c_d(cd_alpha, [qmr[0]], qr[0])\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.c_layer1.parameters() + self.c_layer2.parameters() + self.q_weights\n",
    "    \n",
    "# --- 4. Training - MODIFIED FOR STABILITY ---\n",
    "def get_full_data(n_samples=1000):\n",
    "    try:\n",
    "        df = pd.read_csv(r\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: Please update the path to 'creditcard_data.csv'\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    fraud = df[df['Class'] == 1]\n",
    "    normal = df[df['Class'] == 0]\n",
    "    n_per = n_samples // 2\n",
    "    fraud = resample(fraud, n_samples=n_per, random_state=42)\n",
    "    normal = resample(normal, n_samples=n_per, random_state=42)\n",
    "    df_bal = pd.concat([fraud, normal])\n",
    "    \n",
    "    X = df_bal.drop(['Class', 'Time'], axis=1).values\n",
    "    y = df_bal['Class'].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup\n",
    "X_train, X_test, y_train, y_test = get_full_data(n_samples=10000)\n",
    "if X_train is not None:\n",
    "    model = HybridFraudDetector(n_features=29)\n",
    "    optimizer_params = model.parameters()\n",
    "    epochs = 10\n",
    "    \n",
    "    # --- CHANGE 1: Lower the learning rate ---\n",
    "    # Cross-entropy can produce large gradients, so a smaller LR is needed for stability.\n",
    "    lr = 0.001 \n",
    "    \n",
    "    print(f\"Training with BINARY CROSS-ENTROPY loss...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        start = time.time()\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        \n",
    "        for i in range(0, len(X_train), 64):\n",
    "            batch_idx = indices[i:i+64]\n",
    "            batch_loss = Value(0)\n",
    "            \n",
    "            for idx in batch_idx:\n",
    "                # pred is now a probability p in [0, 1]\n",
    "                p = model.forward(X_train[idx])\n",
    "                y_target = y_train[idx]\n",
    "                \n",
    "                # --- CHANGE FOR CROSS-ENTROPY ---\n",
    "                # Loss = -(y*log(p) + (1-y)*log(1-p))\n",
    "                y = Value(y_target)\n",
    "                term = (y * p.log() + (Value(1) + y.__neg__()) * (Value(1) + p.__neg__()).log()).__neg__()\n",
    "                \n",
    "                batch_loss = batch_loss + term\n",
    "                \n",
    "            batch_loss = batch_loss * Value(1.0/64)\n",
    "            \n",
    "            # Zero Grads & Backprop\n",
    "            for p in optimizer_params: p.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            # --- CHANGE 2: Implement Gradient Clipping ---\n",
    "            # This prevents exploding gradients from destabilizing the training.\n",
    "            clip_value = 1.0 # Do not allow any single gradient to be larger than this.\n",
    "            for p in optimizer_params:\n",
    "                p.grad = np.clip(p.grad, -clip_value, clip_value)\n",
    "\n",
    "            # Update weights\n",
    "            for p in optimizer_params:\n",
    "                p.data -= lr * p.grad\n",
    "                \n",
    "            epoch_loss += batch_loss.data\n",
    "            print(f\"\\rBatch {i//64} | Loss: {batch_loss.data:.4f}\", end=\"\")\n",
    "            \n",
    "        print(f\"\\nEpoch {epoch+1} | Avg Loss: {epoch_loss / (len(X_train)/64):.4f} | Time: {time.time()-start:.1f}s\")\n",
    "        \n",
    "        # Test AUC (still works with probability outputs)\n",
    "        preds = [model.forward(x).data for x in X_test]\n",
    "        try:\n",
    "            print(f\"Test AUC: {roc_auc_score(y_test, preds):.4f}\")\n",
    "        except: pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boson_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
