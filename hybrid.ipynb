{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ea2689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qiskit version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "# print(\"Bosonic Qiskit version:\", bosonic_qiskit.__version__)\n",
    "print(\"Qiskit version:\", qiskit.__version__)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fd6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bosonic_qiskit.util as util\n",
    "import qiskit\n",
    "import qiskit.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51457a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmr = bosonic_qiskit.QumodeRegister(num_qumodes = 1, num_qubits_per_qumode = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b848d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 180.108x284.278 with 1 Axes>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = bosonic_qiskit.CVCircuit(qmr)\n",
    "circuit.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371a9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.cv_initialize([0,0,0,0,0,1,0,0], qmr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f22b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.cv_initialize([1/np.sqrt(2), 1/np.sqrt(2)], qmr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f802e1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 180.31x535.111 with 1 Axes>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmr = bosonic_qiskit.QumodeRegister(\n",
    "    num_qumodes=1,\n",
    "    num_qubits_per_qumode=4\n",
    ")\n",
    "\n",
    "qr = qiskit.QuantumRegister(\n",
    "    size=1\n",
    ")\n",
    "\n",
    "cr = qiskit.ClassicalRegister(\n",
    "    size = 1\n",
    ")\n",
    "\n",
    "circuit = bosonic_qiskit.CVCircuit(qmr, qr, cr)\n",
    "circuit.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad10dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "CircuitError",
     "evalue": "'register size error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCircuitError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCOBYLA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained Weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_minimize.py:744\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    741\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    742\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcobyla\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 744\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    745\u001b[0m                            bounds\u001b[38;5;241m=\u001b[39mbounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcobyqa\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    747\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyqa(fun, x0, args, bounds, constraints, callback,\n\u001b[0;32m    748\u001b[0m                            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_cobyla_py.py:35\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _module_lock:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_cobyla_py.py:278\u001b[0m, in \u001b[0;36m_minimize_cobyla\u001b[1;34m(fun, x0, args, constraints, rhobeg, tol, maxiter, disp, catol, callback, bounds, **unknown_options)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_jac\u001b[39m(x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_jac\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalcfc\u001b[39m(x, con):\n\u001b[0;32m    281\u001b[0m     f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun(x)\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_optimize.py:291\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    287\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:223\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    220\u001b[0m     finite_diff_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_linear_operator\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Initial function evaluation\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Initial gradient evaluation\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ngev \u001b[38;5;241m=\u001b[39m _wrapper_grad(\n\u001b[0;32m    227\u001b[0m     grad,\n\u001b[0;32m    228\u001b[0m     fun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_fun,\n\u001b[0;32m    229\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    230\u001b[0m     finite_diff_options\u001b[38;5;241m=\u001b[39mfinite_diff_options\n\u001b[0;32m    231\u001b[0m )\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:295\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 295\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[0;32m    297\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:21\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m, in \u001b[0;36mcost_function\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     44\u001b[0m training_data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m), (\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)] \n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y_target \u001b[38;5;129;01min\u001b[39;00m training_data:\n\u001b[1;32m---> 47\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrun_bosonic_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m-\u001b[39m y_target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mrun_bosonic_network\u001b[1;34m(params, input_data)\u001b[0m\n\u001b[0;32m     22\u001b[0m circuit\u001b[38;5;241m.\u001b[39mcv_snap(params[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m, qmr, qbr[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 4. Measure\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 5. Simulate\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# FIX: Use *_ to ignore extra return values (like noise data/wigner info)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m state, result, _ \u001b[38;5;241m=\u001b[39m bosonic_qiskit\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39msimulate(circuit)\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\bosonic_qiskit\\circuit.py:1255\u001b[0m, in \u001b[0;36mCVCircuit.cv_measure\u001b[1;34m(self, qargs, cargs)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     cargs \u001b[38;5;241m=\u001b[39m [cargs]\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;66;03m# Discards unnecessary clbits so the user doesn't need to think about how many clbits are needed\u001b[39;00m\n\u001b[1;32m-> 1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqargs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqargs_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\qiskit\\circuit\\quantumcircuit.py:4039\u001b[0m, in \u001b[0;36mQuantumCircuit.measure\u001b[1;34m(self, qubit, cbit)\u001b[0m\n\u001b[0;32m   3950\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Measure a quantum bit (``qubit``) in the Z basis into a classical bit (``cbit``).\u001b[39;00m\n\u001b[0;32m   3951\u001b[0m \n\u001b[0;32m   3952\u001b[0m \u001b[38;5;124;03mWhen a quantum state is measured, a qubit is projected in the computational (Pauli Z) basis\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4035\u001b[0m \n\u001b[0;32m   4036\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4037\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Measure\n\u001b[1;32m-> 4039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mqubit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcbit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\qiskit\\circuit\\quantumcircuit.py:2497\u001b[0m, in \u001b[0;36mQuantumCircuit.append\u001b[1;34m(self, instruction, qargs, cargs, copy)\u001b[0m\n\u001b[0;32m   2491\u001b[0m broadcast_iter \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2492\u001b[0m     operation\u001b[38;5;241m.\u001b[39mbroadcast_arguments(expanded_qargs, expanded_cargs)\n\u001b[0;32m   2493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operation, Instruction)\n\u001b[0;32m   2494\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m Instruction\u001b[38;5;241m.\u001b[39mbroadcast_arguments(operation, expanded_qargs, expanded_cargs)\n\u001b[0;32m   2495\u001b[0m )\n\u001b[0;32m   2496\u001b[0m base_instruction \u001b[38;5;241m=\u001b[39m CircuitInstruction(operation, (), ())\n\u001b[1;32m-> 2497\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qarg, carg \u001b[38;5;129;01min\u001b[39;00m broadcast_iter:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_dups(qarg)\n\u001b[0;32m   2499\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m base_instruction\u001b[38;5;241m.\u001b[39mreplace(qubits\u001b[38;5;241m=\u001b[39mqarg, clbits\u001b[38;5;241m=\u001b[39mcarg)\n",
      "File \u001b[1;32md:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\qiskit\\circuit\\measure.py:53\u001b[0m, in \u001b[0;36mMeasure.broadcast_arguments\u001b[1;34m(self, qargs, cargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m qarg, [each_carg]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CircuitError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister size error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mCircuitError\u001b[0m: 'register size error'"
     ]
    }
   ],
   "source": [
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def run_bosonic_network(params, input_data):\n",
    "    # 1. Setup Circuit\n",
    "    qmr = bosonic_qiskit.QumodeRegister(num_qumodes=1, num_qubits_per_qumode=4)\n",
    "    qbr = qiskit.QuantumRegister(1)\n",
    "    cr = qiskit.ClassicalRegister(1) # Ensure this matches qumode size\n",
    "    \n",
    "    circuit = bosonic_qiskit.CVCircuit(qmr, qbr, cr)\n",
    "\n",
    "    # 2. Input Layer\n",
    "    circuit.cv_d(input_data, qmr) \n",
    "\n",
    "    # 3. Hidden Layer \n",
    "    circuit.cv_sq(params[0], qmr) \n",
    "    circuit.cv_r(params[1], qmr)\n",
    "    \n",
    "    # Apply SNAP gate to Fock state |1>\n",
    "    circuit.cv_snap(params[2], 1, qmr, qbr[0])\n",
    "\n",
    "    # 4. Measure\n",
    "    circuit.cv_measure(qmr, cr)\n",
    "\n",
    "    # 5. Simulate\n",
    "    # FIX: Use *_ to ignore extra return values (like noise data/wigner info)\n",
    "    state, result, _ = bosonic_qiskit.util.simulate(circuit)\n",
    "    \n",
    "    counts = result.get_counts()\n",
    "    fock_counts = bosonic_qiskit.util.cv_fockcounts(counts, qmr)\n",
    "    \n",
    "    total_shots = sum(fock_counts.values())\n",
    "    if total_shots > 0:\n",
    "        avg_n = sum(n * c for n, c in fock_counts.items()) / total_shots\n",
    "    else:\n",
    "        avg_n = 0\n",
    "        \n",
    "    return avg_n\n",
    "\n",
    "def cost_function(params):\n",
    "    total_error = 0\n",
    "    training_data = [(0.5, 1.0), (0.1, 0.0)] \n",
    "    \n",
    "    for x, y_target in training_data:\n",
    "        y_pred = run_bosonic_network(params, x)\n",
    "        total_error += (y_pred - y_target) ** 2\n",
    "    \n",
    "    print(f\"Params: {params}, Error: {total_error}\")\n",
    "    return total_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    initial_weights = [0.1, 0.5, 0.2]\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Run optimization\n",
    "    result = minimize(cost_function, initial_weights, method='COBYLA', tol=0.01)\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    print(\"Trained Weights:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmr1 = bosonic_qiskit.QumodeRegister(num_qumodes=1, num_qubits_per_qumode=4)\n",
    "qbr1 = qiskit.QuantumRegister(1)\n",
    "cr1 = qiskit.ClassicalRegister(1)\n",
    "\n",
    "circuit1 = bosonic_qiskit.CVCircuit(qmr1, qbr1, cr1)\n",
    "circuit1.cv_initialize(0, qmr1[0])\n",
    "\n",
    "alpha1 = 1\n",
    "circuit1.h(qbr1[0])\n",
    "circuit1.cv_c_d(alpha1, qmr1[0], qbr1[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168e220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Manual Gradients on 43 parameters.\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 1: Loss=0.100 | AUC=0.500\n",
      "  >>> New Best AUC! Model Saved.\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 2: Loss=0.100 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 3: Loss=0.200 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 4: Loss=0.100 | AUC=0.500\n",
      "  Computing Gradients for 43 parameters........ Done.\n",
      "Epoch 5: Loss=0.100 | AUC=0.500\n",
      "\n",
      "Final Evaluation on Test Set...\n",
      "Final Test ROC AUC: 0.6250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, roc_curve\n",
    ")\n",
    "\n",
    "# System and Warning Handling\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA GENERATION (Simulated Fraud Data)\n",
    "# ==========================================\n",
    "def get_data():\n",
    "    # Reduced features to 4 for speed (Manual Gradients are slow!)\n",
    "    X, y = make_classification(n_samples=300, n_features=4, n_informative=4, \n",
    "                               n_redundant=0, weights=[0.9, 0.1], random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data()\n",
    "\n",
    "# ==========================================\n",
    "# 2. HYBRID MODEL (Forward Pass)\n",
    "# ==========================================\n",
    "def apply_kerr(circuit, kerr_param, qumode, cutoff):\n",
    "    \"\"\"Manual Kerr Gate using SNAP\"\"\"\n",
    "    for n in range(cutoff):\n",
    "        phase = kerr_param * (n ** 2)\n",
    "        if abs(phase) > 1e-4:\n",
    "            circuit.cv_snap(phase, n, qumode, None)\n",
    "\n",
    "def get_probs(result):\n",
    "    \"\"\"Extract probabilities for Class 0 (|1,0>) and Class 1 (|0,1>)\"\"\"\n",
    "    counts = result.get_counts()\n",
    "    total_shots = sum(counts.values())\n",
    "    p_10, p_01 = 0, 0\n",
    "    \n",
    "    for bitstr, count in counts.items():\n",
    "        val = int(bitstr.replace(\" \", \"\"), 2)\n",
    "        # Assuming 2 qubits per mode (Cutoff=4)\n",
    "        n_a = val & 3\n",
    "        n_b = (val >> 2) & 3\n",
    "        \n",
    "        if n_a == 1 and n_b == 0: p_10 += count\n",
    "        elif n_a == 0 and n_b == 1: p_01 += count\n",
    "            \n",
    "    return (p_10/total_shots, p_01/total_shots) if total_shots > 0 else (0,0)\n",
    "\n",
    "def hybrid_forward(params, x_input):\n",
    "    # --- Classical NN (4 inputs -> 8 outputs) ---\n",
    "    # Reduced size for speed\n",
    "    n_in, n_out = 4, 8\n",
    "    n_classical = n_in * n_out + n_out\n",
    "    \n",
    "    w = params[:n_in*n_out].reshape(n_in, n_out)\n",
    "    b = params[n_in*n_out : n_classical]\n",
    "    \n",
    "    # NN Output used as parameters for the encoding layer\n",
    "    nn_out = np.tanh(np.dot(x_input, w) + b)\n",
    "    \n",
    "    # --- Quantum Circuit ---\n",
    "    qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=2)\n",
    "    cr = qiskit.ClassicalRegister(4)\n",
    "    circuit = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "    \n",
    "    # Encoding (Driven by NN output)\n",
    "    # Using first 8 params for: Sq(0), Sq(1), BS, R(0), R(1), D(0), D(1), Kerr(0)\n",
    "    circuit.cv_sq(nn_out[0], qmr[0])\n",
    "    circuit.cv_sq(nn_out[1], qmr[1])\n",
    "    circuit.cv_bs(nn_out[2], qmr[0], qmr[1])\n",
    "    circuit.cv_r(nn_out[3], qmr[0])\n",
    "    circuit.cv_d(nn_out[4], qmr[0])\n",
    "    apply_kerr(circuit, nn_out[5], qmr[0], 4)\n",
    "    \n",
    "    # Variational Layer (Trainable Quantum Params)\n",
    "    q_params = params[n_classical:]\n",
    "    circuit.cv_bs(q_params[0], qmr[0], qmr[1])\n",
    "    circuit.cv_r(q_params[1], qmr[0])\n",
    "    circuit.cv_snap(q_params[2], 1, qmr[0], None) # Non-linearity\n",
    "\n",
    "    circuit.cv_measure(qmr, cr)\n",
    "    \n",
    "    # Simulate\n",
    "    try:\n",
    "        _, res, *_ = bosonic_qiskit.util.simulate(circuit, shots=100)\n",
    "        return get_probs(res)\n",
    "    except:\n",
    "        return 0.5, 0.5\n",
    "\n",
    "# ==========================================\n",
    "# 3. MANUAL GRADIENT CALCULATION\n",
    "# ==========================================\n",
    "def get_batch_loss(params, X_batch, y_batch):\n",
    "    loss = 0\n",
    "    scores = []\n",
    "    \n",
    "    for x, y in zip(X_batch, y_batch):\n",
    "        p_10, p_01 = hybrid_forward(params, x)\n",
    "        \n",
    "        # Loss: MSE distance from ideal probability\n",
    "        # Class 0 -> Target p_10=1.0 (p_01=0.0)\n",
    "        # Class 1 -> Target p_01=1.0 \n",
    "        target = 1.0 if y == 1 else 0.0\n",
    "        pred = p_01\n",
    "        \n",
    "        loss += (pred - target) ** 2\n",
    "        scores.append(pred) # Score for AUC is p_01 (Fraud Probability)\n",
    "        \n",
    "    return loss / len(y_batch), scores\n",
    "\n",
    "def compute_gradients_finite_diff(params, X_batch, y_batch, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Manually calculates gradient for EACH parameter.\n",
    "    Run time = 2 * N_params * Batch_Size * Simulator_Time\n",
    "    \"\"\"\n",
    "    grads = np.zeros_like(params)\n",
    "    \n",
    "    # Base loss (optional, for debugging)\n",
    "    # base_loss, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "    \n",
    "    print(f\"  Computing Gradients for {len(params)} parameters...\", end=\"\", flush=True)\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        # Shift Right\n",
    "        params[i] += epsilon\n",
    "        loss_plus, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "        \n",
    "        # Shift Left\n",
    "        params[i] -= 2 * epsilon # Move back to -epsilon\n",
    "        loss_minus, _ = get_batch_loss(params, X_batch, y_batch)\n",
    "        \n",
    "        # Reset\n",
    "        params[i] += epsilon \n",
    "        \n",
    "        # Central Difference\n",
    "        grads[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        if i % 10 == 0: print(\".\", end=\"\", flush=True)\n",
    "            \n",
    "    print(\" Done.\")\n",
    "    return grads\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP (MAXIMIZE AUC)\n",
    "# ==========================================\n",
    "# Setup Parameters: 4*8 (Weights) + 8 (Bias) + 3 (Quantum) = 43 params\n",
    "n_params = (4 * 8) + 8 + 3\n",
    "weights = np.random.normal(0, 0.1, n_params)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 5 # Small number due to simulation time\n",
    "batch_size = 4 \n",
    "\n",
    "best_val_auc = 0\n",
    "best_weights = None\n",
    "\n",
    "print(f\"Starting Training with Manual Gradients on {n_params} parameters.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Mini-Batch Training\n",
    "    indices = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "    X_batch = X_train[indices]\n",
    "    y_batch = y_train[indices]\n",
    "    \n",
    "    # 2. Compute Manual Gradients\n",
    "    grads = compute_gradients_finite_diff(weights, X_batch, y_batch)\n",
    "    \n",
    "    # 3. Update Weights (Gradient Descent)\n",
    "    weights = weights - (learning_rate * grads)\n",
    "    \n",
    "    # 4. Validation & AUC Check\n",
    "    # We evaluate on a hold-out set to find the Best AUC model\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    val_indices = np.random.choice(len(X_test), 10, replace=False) # Subset for speed\n",
    "    \n",
    "    for idx in val_indices:\n",
    "        p_10, p_01 = hybrid_forward(weights, X_test[idx])\n",
    "        val_preds.append(p_01)\n",
    "        val_loss += (p_01 - (1 if y_test[idx]==1 else 0))**2\n",
    "        \n",
    "    try:\n",
    "        # Calculate AUC (needs both classes present in the batch)\n",
    "        if len(set(y_test[val_indices])) > 1:\n",
    "            current_auc = roc_auc_score(y_test[val_indices], val_preds)\n",
    "        else:\n",
    "            current_auc = 0.5\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={val_loss/10:.3f} | AUC={current_auc:.3f}\")\n",
    "        \n",
    "        # SAVE THE MODEL WITH BEST AUC\n",
    "        if current_auc > best_val_auc:\n",
    "            best_val_auc = current_auc\n",
    "            best_weights = weights.copy()\n",
    "            print(\"  >>> New Best AUC! Model Saved.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Metric Calc Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FINAL TEST\n",
    "# ==========================================\n",
    "print(\"\\nFinal Evaluation on Test Set...\")\n",
    "final_preds = []\n",
    "for x in X_test[:30]: # Limit for speed\n",
    "    _, p_01 = hybrid_forward(best_weights if best_weights is not None else weights, x)\n",
    "    final_preds.append(p_01)\n",
    "\n",
    "final_auc = roc_auc_score(y_test[:30], final_preds)\n",
    "print(f\"Final Test ROC AUC: {final_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc795c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0000\n",
      "Loss:       1.0000\n",
      "Grads:      Sq=0.0000, Rot=0.0000\n",
      "New Params: Sq=0.1000, Rot=0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "\n",
    "# --- 1. The Micrograd Engine (Minimal Version) ---\n",
    "class Value:\n",
    "    \"\"\"Stores a scalar and its gradient.\"\"\"\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "# --- 2. The Differentiable Gate Operation ---\n",
    "class QuantumOp(Value):\n",
    "    \"\"\"\n",
    "    Represents the RESULT of a quantum circuit execution.\n",
    "    It connects the Gate Parameters (parents) to the Measurement (output).\n",
    "    \"\"\"\n",
    "    def __init__(self, circuit_builder, params, input_val):\n",
    "        self.params = params  # List of Value objects (The Gate Angles)\n",
    "        self.input_val = input_val\n",
    "        self.circuit_builder = circuit_builder\n",
    "        \n",
    "        # Forward Pass: Run the circuit once to get the value\n",
    "        val = self._run_simulation([p.data for p in self.params])\n",
    "        \n",
    "        super().__init__(val, tuple(params), 'QuantumGate')\n",
    "\n",
    "    def _run_simulation(self, numeric_params):\n",
    "        \"\"\"Helper to run the actual simulator\"\"\"\n",
    "        # Rebuild circuit with specific numeric values\n",
    "        qmr = bosonic_qiskit.QumodeRegister(1, 2)\n",
    "        cr = qiskit.ClassicalRegister(2)\n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "        \n",
    "        # Call the user's circuit definition\n",
    "        self.circuit_builder(circ, numeric_params, self.input_val, qmr)\n",
    "        \n",
    "        circ.cv_measure(qmr, cr)\n",
    "        # Suppress output, run sim\n",
    "        try:\n",
    "            _, res, *_ = bosonic_qiskit.util.simulate(circ, shots=100)\n",
    "            counts = res.get_counts()\n",
    "            # Return Prob(|1>)\n",
    "            return counts.get('1', 0) / 100.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _backward_step(self):\n",
    "        \"\"\"The Parameter Shift Rule Implementation\"\"\"\n",
    "        shift = np.pi / 2\n",
    "        \n",
    "        for i, param in enumerate(self.params):\n",
    "            # 1. Shift +\n",
    "            p_vals = [p.data for p in self.params]\n",
    "            p_vals[i] += shift\n",
    "            out_plus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # 2. Shift -\n",
    "            p_vals[i] -= 2 * shift # Go from +s to -s\n",
    "            out_minus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # 3. Gradient\n",
    "            local_grad = (out_plus - out_minus) / 2.0\n",
    "            \n",
    "            # Chain Rule\n",
    "            param.grad += local_grad * self.grad\n",
    "\n",
    "# --- 3. User Code: Defining the Circuit ---\n",
    "\n",
    "def my_quantum_layer(circuit, numeric_params, x_input, qmr):\n",
    "    \"\"\"\n",
    "    The physical definition of the circuit.\n",
    "    numeric_params[0] -> Squeezing\n",
    "    numeric_params[1] -> Rotation\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    circuit.cv_d(x_input, qmr[0])\n",
    "    \n",
    "    # Variational Gates\n",
    "    circuit.cv_sq(numeric_params[0], qmr[0]) \n",
    "    circuit.cv_r(numeric_params[1], qmr[0])\n",
    "\n",
    "# --- 4. Training Script ---\n",
    "\n",
    "# Init Trainable Weights\n",
    "w_squeeze = Value(0.1)\n",
    "w_rotation = Value(0.5)\n",
    "params = [w_squeeze, w_rotation]\n",
    "\n",
    "# Data\n",
    "x = 0.5\n",
    "y_target = 1.0\n",
    "\n",
    "# Forward\n",
    "# The 'QuantumOp' acts like a node in the graph. \n",
    "# It takes 'params' as parents and produces a 'pred' value.\n",
    "pred = QuantumOp(my_quantum_layer, params, x)\n",
    "\n",
    "# Loss (MSE)\n",
    "error = pred + Value(-y_target)\n",
    "loss = error * error\n",
    "\n",
    "# Backward\n",
    "# This triggers the _backward_step inside QuantumOp, which\n",
    "# runs the simulator multiple times to shift the parameters.\n",
    "loss.backward()\n",
    "\n",
    "# Update\n",
    "lr = 0.1\n",
    "for p in params:\n",
    "    p.data -= lr * p.grad\n",
    "\n",
    "print(f\"Prediction: {pred.data:.4f}\")\n",
    "print(f\"Loss:       {loss.data:.4f}\")\n",
    "print(f\"Grads:      Sq={w_squeeze.grad:.4f}, Rot={w_rotation.grad:.4f}\")\n",
    "print(f\"New Params: Sq={w_squeeze.data:.4f}, Rot={w_rotation.data:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9135723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "\n",
    "# --- 1. Autograd Engine (Value Class) ---\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = 0\n",
    "\n",
    "# --- 2. Quantum Operation Node ---\n",
    "class QuantumOp(Value):\n",
    "    def __init__(self, circuit_builder, params, input_val):\n",
    "        self.params = params \n",
    "        self.input_val = input_val\n",
    "        self.circuit_builder = circuit_builder\n",
    "        \n",
    "        # Forward Pass\n",
    "        val = self._run_simulation([p.data for p in self.params])\n",
    "        super().__init__(val, tuple(params), 'QuantumGate')\n",
    "\n",
    "    def _run_simulation(self, numeric_params):\n",
    "        # Setup: 2 Modes, 2 Qubits per mode (Cutoff=4)\n",
    "        qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=2)\n",
    "        cr = qiskit.ClassicalRegister(4) \n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "        \n",
    "        self.circuit_builder(circ, numeric_params, self.input_val, qmr)\n",
    "        circ.cv_measure(qmr, cr)\n",
    "        \n",
    "        try:\n",
    "            # Low shots (50) for speed during training\n",
    "            _, res, *_ = bosonic_qiskit.util.simulate(circ, shots=50)\n",
    "            counts = res.get_counts()\n",
    "            \n",
    "            # Metric: Prob of |0,1> (Fraud State)\n",
    "            prob_fraud = 0\n",
    "            for bitstr, count in counts.items():\n",
    "                val = int(bitstr.replace(\" \", \"\"), 2)\n",
    "                # Check for 1 photon in Mode 1 (second mode)\n",
    "                # Mask: last 2 bits are Mode 0, first 2 bits are Mode 1\n",
    "                n_mode_1 = (val >> 2) & 3\n",
    "                if n_mode_1 >= 1: \n",
    "                    prob_fraud += count\n",
    "            \n",
    "            return prob_fraud / 50.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _backward(self):\n",
    "        h = 1e-2 # Finite difference step\n",
    "        \n",
    "        for i, param in enumerate(self.params):\n",
    "            p_vals = [p.data for p in self.params]\n",
    "            \n",
    "            # Shift Right\n",
    "            p_vals[i] += h\n",
    "            out_plus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # Shift Left\n",
    "            p_vals[i] -= 2 * h\n",
    "            out_minus = self._run_simulation(p_vals)\n",
    "            \n",
    "            # Gradient\n",
    "            local_grad = (out_plus - out_minus) / (2 * h)\n",
    "            param.grad += self.grad * local_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b1607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Starting Training on 800 samples...\n",
      "Epoch 1: Processed 800/800 samples...\n",
      "Epoch 1 Completed in 33.8s | Avg Loss: 0.4934\n",
      "Test ROC AUC: 0.2667\n",
      "------------------------------\n",
      "Epoch 2: Processed 800/800 samples...\n",
      "Epoch 2 Completed in 33.3s | Avg Loss: 0.4944\n",
      "Test ROC AUC: 0.4667\n",
      "------------------------------\n",
      "Epoch 3: Processed 800/800 samples...\n",
      "Epoch 3 Completed in 41.6s | Avg Loss: 0.4944\n",
      "Test ROC AUC: 0.6000\n",
      "------------------------------\n",
      "Epoch 4: Processed 800/800 samples...\n",
      "Epoch 4 Completed in 40.5s | Avg Loss: 0.4946\n",
      "Test ROC AUC: 0.4733\n",
      "------------------------------\n",
      "Epoch 5: Processed 800/800 samples...\n",
      "Epoch 5 Completed in 38.1s | Avg Loss: 0.4950\n",
      "Test ROC AUC: 0.5400\n",
      "------------------------------\n",
      "Epoch 6: Processed 800/800 samples...\n",
      "Epoch 6 Completed in 35.8s | Avg Loss: 0.4949\n",
      "Test ROC AUC: 0.3867\n",
      "------------------------------\n",
      "Epoch 7: Processed 800/800 samples...\n",
      "Epoch 7 Completed in 34.4s | Avg Loss: 0.4935\n",
      "Test ROC AUC: 0.4733\n",
      "------------------------------\n",
      "Epoch 8: Processed 800/800 samples...\n",
      "Epoch 8 Completed in 35.8s | Avg Loss: 0.4942\n",
      "Test ROC AUC: 0.4267\n",
      "------------------------------\n",
      "Epoch 9: Processed 800/800 samples...\n",
      "Epoch 9 Completed in 37.7s | Avg Loss: 0.4940\n",
      "Test ROC AUC: 0.4400\n",
      "------------------------------\n",
      "Epoch 10: Processed 800/800 samples...\n",
      "Epoch 10 Completed in 35.2s | Avg Loss: 0.4924\n",
      "Test ROC AUC: 0.3333\n",
      "------------------------------\n",
      "Epoch 11: Processed 800/800 samples...\n",
      "Epoch 11 Completed in 34.7s | Avg Loss: 0.4939\n",
      "Test ROC AUC: 0.2200\n",
      "------------------------------\n",
      "Epoch 12: Processed 800/800 samples...\n",
      "Epoch 12 Completed in 34.3s | Avg Loss: 0.4946\n",
      "Test ROC AUC: 0.2933\n",
      "------------------------------\n",
      "Epoch 13: Processed 800/800 samples...\n",
      "Epoch 13 Completed in 34.5s | Avg Loss: 0.4939\n",
      "Test ROC AUC: 0.2867\n",
      "------------------------------\n",
      "Epoch 14: Processed 800/800 samples...\n",
      "Epoch 14 Completed in 35.8s | Avg Loss: 0.4941\n",
      "Test ROC AUC: 0.4400\n",
      "------------------------------\n",
      "Epoch 15: Processed 800/800 samples...\n",
      "Epoch 15 Completed in 34.6s | Avg Loss: 0.4924\n",
      "Test ROC AUC: 0.3733\n",
      "------------------------------\n",
      "Final Weights:\n",
      "[0.01, 0.05, 0.1, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "# --- A. Data Loading (Same as before) ---\n",
    "def get_data_for_training(filepath=r\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\", n_samples=200):\n",
    "    # Load and balance\n",
    "    df = pd.read_csv(filepath)\n",
    "    fraud = df[df['Class'] == 1]\n",
    "    normal = df[df['Class'] == 0]\n",
    "    \n",
    "    n_per = n_samples // 2\n",
    "    fraud = resample(fraud, n_samples=n_per, random_state=42)\n",
    "    normal = resample(normal, n_samples=n_per, random_state=42)\n",
    "    df_balanced = pd.concat([fraud, normal])\n",
    "    \n",
    "    X = df_balanced.drop(['Class', 'Time'], axis=1)\n",
    "    y = df_balanced['Class'].values\n",
    "    \n",
    "    # Scale & PCA\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Reduce to just 1 feature for this demo (Speed!)\n",
    "    # Or use 2 features if you have patience.\n",
    "    pca = PCA(n_components=2) \n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- B. Define Quantum Circuit ---\n",
    "def fraud_circuit(circuit, params, x_input, qmr):\n",
    "    \"\"\"\n",
    "    params[0]: Squeezing\n",
    "    params[1]: Rotation\n",
    "    params[2]: Beam Splitter\n",
    "    params[3]: Displacement bias\n",
    "    \"\"\"\n",
    "    # Encoder: Data -> Displacement on Mode 0\n",
    "    # x_input is an array/list from PCA\n",
    "    val = x_input[0] \n",
    "    circuit.cv_d(val, qmr[0])\n",
    "    \n",
    "    # Trainable Variational Layer\n",
    "    circuit.cv_sq(params[0], qmr[0])\n",
    "    circuit.cv_r(params[1], qmr[0])\n",
    "    \n",
    "    # Coupling to Mode 1 (where we measure fraud)\n",
    "    circuit.cv_bs(params[2], qmr[0], qmr[1])\n",
    "    \n",
    "    # Bias displacement\n",
    "    circuit.cv_d(params[3], qmr[1])\n",
    "\n",
    "# --- C. Training Loop ---\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"Loading Data...\")\n",
    "# Adjust 'n_samples' based on your patience. 100 samples ~ 5 mins training.\n",
    "X_train, X_test, y_train, y_test = get_data_for_training(filepath=\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\", n_samples=1000)\n",
    "\n",
    "# 2. Init Weights (Value objects)\n",
    "w_sq = Value(0.01)\n",
    "w_rot = Value(0.05)\n",
    "w_bs = Value(0.1)\n",
    "w_bias = Value(0.0)\n",
    "params = [w_sq, w_rot, w_bs, w_bias]\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 15\n",
    "batch_size = 20\n",
    "learning_rate = 0.05\n",
    "\n",
    "print(f\"Starting Training on {len(X_train)} samples...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Mini-Batch Loop\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i : i + batch_size]\n",
    "        batch_loss = Value(0) # Accumulator for batch loss\n",
    "        \n",
    "        # 1. Forward Pass (Accumulate Loss)\n",
    "        for idx in batch_idx:\n",
    "            x_in = X_train[idx]\n",
    "            y_target = y_train[idx] # 0 or 1\n",
    "            \n",
    "            # Build Graph\n",
    "            pred = QuantumOp(fraud_circuit, params, x_in)\n",
    "            \n",
    "            # MSE Loss: (pred - target)^2\n",
    "            diff = pred + Value(-float(y_target))\n",
    "            sample_loss = diff * diff\n",
    "            \n",
    "            batch_loss = batch_loss + sample_loss\n",
    "        \n",
    "        # Average the loss\n",
    "        batch_loss = batch_loss * Value(1.0 / batch_size)\n",
    "        epoch_loss += batch_loss.data\n",
    "        \n",
    "        # 2. Zero Gradients\n",
    "        for p in params: p.zero_grad()\n",
    "            \n",
    "        # 3. Backward Pass\n",
    "        # This triggers simulations for the whole batch\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # 4. Update\n",
    "        for p in params:\n",
    "            p.data -= learning_rate * p.grad\n",
    "            \n",
    "        print(f\"\\rEpoch {epoch+1}: Processed {i+len(batch_idx)}/{len(X_train)} samples...\", end=\"\")\n",
    "    \n",
    "    # Validation at end of epoch\n",
    "    print(f\"\\nEpoch {epoch+1} Completed in {time.time()-start_time:.1f}s | Avg Loss: {epoch_loss / (len(X_train)/batch_size):.4f}\")\n",
    "    \n",
    "    # Check AUC on Test Set (subset)\n",
    "    test_preds = []\n",
    "    for x in X_test[:20]:\n",
    "        # Fast inference (no gradients needed)\n",
    "        # We can just use the forward pass logic of QuantumOp manually or create nodes\n",
    "        p = QuantumOp(fraud_circuit, params, x).data\n",
    "        test_preds.append(p)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test[:20], test_preds)\n",
    "        print(f\"Test ROC AUC: {auc:.4f}\")\n",
    "    except:\n",
    "        print(\"AUC Calc Error (likely single class in batch)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"Final Weights:\")\n",
    "print([p.data for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with MEAN PHOTON metric...\n",
      "Batch 0 | Loss: 1.0413"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\sparse\\_index.py:168: SparseEfficiencyWarning: Changing the sparsity structure of a csc_array is expensive. lil and dok are more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 124 | Loss: 0.9934\n",
      "Epoch 1 | Avg Loss: 1.0364 | Time: 1222.4s\n",
      "Test AUC: 0.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\Fraud_Detection_QML\\boson_env\\lib\\site-packages\\scipy\\sparse\\_index.py:168: SparseEfficiencyWarning: Changing the sparsity structure of a csc_array is expensive. lil and dok are more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63 | Loss: 1.0709"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bosonic_qiskit\n",
    "import qiskit\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- 1. Autograd Engine (Value Class) ---\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * Value(-1)\n",
    "\n",
    "    def log(self):\n",
    "        val = self.data\n",
    "        # Clip to prevent log(0) = -inf\n",
    "        if val < 1e-7: val = 1e-7\n",
    "        if val > 1.0 - 1e-7: val = 1.0 - 1e-7\n",
    "        \n",
    "        out = Value(np.log(val), (self,), 'log')\n",
    "        def _backward():\n",
    "            self.grad += (1.0 / val) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def tanh(self):\n",
    "        # Tanh activation allows negative values (Phase Space!)\n",
    "        x = self.data\n",
    "        t = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        self.grad = 0\n",
    "\n",
    "# --- 2. Improved Quantum Op (Mean Photon Measurement) ---\n",
    "# --- REPLACEMENT CLASS ---\n",
    "class QuantumOp(Value):\n",
    "    def __init__(self, circuit_builder, weights, inputs):\n",
    "        self.weights = weights\n",
    "        self.inputs = inputs\n",
    "        self.circuit_builder = circuit_builder\n",
    "        parents = tuple(weights + inputs)\n",
    "        \n",
    "        # Forward\n",
    "        w_vals = [w.data for w in self.weights]\n",
    "        i_vals = [i.data for i in self.inputs]\n",
    "        val = self._run_simulation(w_vals, i_vals)\n",
    "        super().__init__(val, parents, 'QuantumHybrid')\n",
    "\n",
    "    def _run_simulation(self, w_vals, i_vals):\n",
    "        # 1. Setup Circuit\n",
    "        qmr = bosonic_qiskit.QumodeRegister(num_qumodes=2, num_qubits_per_qumode=2)\n",
    "        cr = qiskit.ClassicalRegister(2) \n",
    "        circ = bosonic_qiskit.CVCircuit(qmr, cr)\n",
    "        \n",
    "        # 2. Build User Circuit\n",
    "        self.circuit_builder(circ, w_vals, i_vals, qmr)\n",
    "        \n",
    "        # 3. CRITICAL CHANGE: Do NOT add cv_measure here.\n",
    "        # We want the state vector, not the measurement counts.\n",
    "        \n",
    "        try:\n",
    "            state, _, _ = bosonic_qiskit.util.simulate(circ)\n",
    "            probs = state.probabilities()\n",
    "            \n",
    "            # Index Logic for Cutoff = 4\n",
    "            # |0,1> (Fraud Target) -> Index 1\n",
    "            # |1,0> (Normal Target) -> Index 4 \n",
    "            \n",
    "            p_fraud  = probs[1] # Probability of |0,1>\n",
    "            p_normal = probs[4] # Probability of |1,0>\n",
    "            \n",
    "            # We pack them into a single float for the Value engine \n",
    "            # (We will unpack them in the loss loop, this is a bit hacky but works for scalar Value class)\n",
    "            # A better way is to just return p_fraud and assume p_normal is roughly (1-p_fraud)\n",
    "            \n",
    "            # Let's return P(Fraud |0,1>) - P(Normal |1,0>)\n",
    "            # If Result is 1.0 -> 100% Fraud\n",
    "            # If Result is -1.0 -> 100% Normal\n",
    "            return p_fraud - p_normal\n",
    "            \n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _backward(self):\n",
    "        # We can use a much smaller 'h' now because there is NO noise!\n",
    "        h = 0.001 \n",
    "        \n",
    "        # (The rest of the backward logic remains exactly the same)\n",
    "        w_numerics = [w.data for w in self.weights]\n",
    "        i_numerics = [i.data for i in self.inputs]\n",
    "        \n",
    "        for idx, w in enumerate(self.weights):\n",
    "            w_copy = w_numerics.copy()\n",
    "            w_copy[idx] += h\n",
    "            out_plus = self._run_simulation(w_copy, i_numerics)\n",
    "            \n",
    "            w_copy[idx] -= 2*h\n",
    "            out_minus = self._run_simulation(w_copy, i_numerics)\n",
    "            \n",
    "            grad = (out_plus - out_minus) / (2*h)\n",
    "            w.grad += self.grad * grad\n",
    "\n",
    "        for idx, inp in enumerate(self.inputs):\n",
    "            i_copy = i_numerics.copy()\n",
    "            i_copy[idx] += h\n",
    "            out_plus = self._run_simulation(w_numerics, i_copy)\n",
    "            i_copy[idx] -= 2*h\n",
    "            out_minus = self._run_simulation(w_numerics, i_copy)\n",
    "            grad = (out_plus - out_minus) / (2*h)\n",
    "            inp.grad += self.grad * grad\n",
    "\n",
    "# --- 3. Hybrid Model with Tanh & Strong Init ---\n",
    "class LinearLayer:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # Stronger Initialization (0.2 scale) to ensure signal flow\n",
    "        self.weights = [[Value(random.gauss(0, 0.2)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "        # Bias towards 0.5 so we start away from vacuum\n",
    "        self.bias = [Value(0.1) for _ in range(n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for j in range(len(self.bias)):\n",
    "            act = self.bias[j]\n",
    "            for i in range(len(x)):\n",
    "                act = act + x[i] * self.weights[i][j]\n",
    "            out.append(act)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for row in self.weights for p in row] + self.bias\n",
    "\n",
    "import cmath\n",
    "\n",
    "class HybridFraudDetector:\n",
    "    def __init__(self, n_features):\n",
    "        # --- 1. Classical Part ---\n",
    "        # Compress 29 features -> 14 parameters for the \"Input Encoding Layer\"\n",
    "        # The SF code uses indices 0 to 13 from 'output_layer', so we need 14 outputs.\n",
    "        self.c_layer1 = LinearLayer(n_features, 20)\n",
    "        self.c_layer2 = LinearLayer(20, 14) \n",
    "        \n",
    "        # --- 2. Quantum Part (Variational Layer) ---\n",
    "        # The 'qnn_layer' function in your snippet implies roughly 18 parameters:\n",
    "        # BS1(2) + R1(2) + Sq(4) + BS2(2) + R2(2) + Disp(4) + Kerr(2)\n",
    "        # We initialize 18 trainable weights.\n",
    "        self.q_weights = []\n",
    "        for i in range(18):\n",
    "            val = random.uniform(-0.1, 0.1) \n",
    "            self.q_weights.append(Value(val))\n",
    "            \n",
    "    def forward(self, x_raw):\n",
    "        x_vals = [Value(xi) for xi in x_raw]\n",
    "        \n",
    "        # Classical Pass (Generates the 14 parameters for Input Layer)\n",
    "        h1 = self.c_layer1(x_vals)\n",
    "        h1_act = [h.tanh() for h in h1]\n",
    "        \n",
    "        # These 14 values act as the \"output_layer\" in your SF code\n",
    "        encoding_params = self.c_layer2(h1_act) \n",
    "        \n",
    "        # Quantum Pass\n",
    "        out = QuantumOp(self.quantum_circuit, self.q_weights, encoding_params)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantum_circuit(circuit, weights, inputs, qmr):\n",
    "        \"\"\"\n",
    "        Implements the exact structure of 'input_qnn_layer' + 'qnn_layer'\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of 14 values (from Classical NN) -> 'output_layer'\n",
    "            weights: List of 18 values (Trainable) -> 'variational vars'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Helper to convert (mag, phase) -> complex\n",
    "        # We use simple float math here as the simulator handles the complex casting\n",
    "        def to_complex(mag, phase):\n",
    "            return mag * cmath.exp(1j * phase)\n",
    "\n",
    "        # ==========================================\n",
    "        # 1. INPUT ENCODING LAYER (Matches input_qnn_layer)\n",
    "        # ==========================================\n",
    "        # Inputs correspond to 'output_layer' indices 0-13\n",
    "        \n",
    "        # Squeezing: Sgate(layer[:,0], layer[:,1])\n",
    "        sq_0 = to_complex(inputs[0], inputs[1])\n",
    "        sq_1 = to_complex(inputs[2], inputs[3])\n",
    "        circuit.cv_sq(sq_0, qmr[0])\n",
    "        circuit.cv_sq(sq_1, qmr[1])\n",
    "        \n",
    "        # Beam Splitter: BSgate(layer[:,4], layer[:,5])\n",
    "        # Bosonic Qiskit BS takes (theta, phi, q1, q2)\n",
    "        circuit.cv_bs(inputs[4], qmr[0], qmr[1])\n",
    "        \n",
    "        # Rotation: Rgate(layer[:,6])\n",
    "        circuit.cv_r(inputs[6], qmr[0])\n",
    "        circuit.cv_r(inputs[7], qmr[1])\n",
    "        \n",
    "        # Displacement: Dgate(layer[:,8], layer[:,9])\n",
    "        d_0 = to_complex(inputs[8], inputs[9])\n",
    "        d_1 = to_complex(inputs[10], inputs[11])\n",
    "        circuit.cv_d(d_0, qmr[0])\n",
    "        circuit.cv_d(d_1, qmr[1])\n",
    "        \n",
    "        # Kerr Gates (Indices 12, 13) are omitted as they are not standard in this backend\n",
    "        # However, the Squeezing/Displacement above provides the necessary non-linearity.\n",
    "\n",
    "        # ==========================================\n",
    "        # 2. VARIATIONAL LAYER (Matches qnn_layer)\n",
    "        # ==========================================\n",
    "        # We map the 18 weights to the gate sequence\n",
    "        \n",
    "        # First Beam Splitter (w0, w1)\n",
    "        circuit.cv_bs(weights[0], qmr[0], qmr[1])\n",
    "        \n",
    "        # First Phase Shifters (w2, w3)\n",
    "        circuit.cv_r(weights[2], qmr[0])\n",
    "        circuit.cv_r(weights[3], qmr[1])\n",
    "        \n",
    "        # Squeezing (w4..w7) -> (mag0, phi0, mag1, phi1)\n",
    "        s_var_0 = to_complex(weights[4], weights[5])\n",
    "        s_var_1 = to_complex(weights[6], weights[7])\n",
    "        circuit.cv_sq(s_var_0, qmr[0])\n",
    "        circuit.cv_sq(s_var_1, qmr[1])\n",
    "        \n",
    "        # Second Beam Splitter (w8, w9)\n",
    "        circuit.cv_bs(weights[8], qmr[0], qmr[1])\n",
    "        \n",
    "        # Second Phase Shifters (w10, w11)\n",
    "        circuit.cv_r(weights[10], qmr[0])\n",
    "        circuit.cv_r(weights[11], qmr[1])\n",
    "        \n",
    "        # Displacement (w12..w15) -> (mag0, phi0, mag1, phi1)\n",
    "        d_var_0 = to_complex(weights[12], weights[13])\n",
    "        d_var_1 = to_complex(weights[14], weights[15])\n",
    "        circuit.cv_d(d_var_0, qmr[0])\n",
    "        circuit.cv_d(d_var_1, qmr[1])\n",
    "        \n",
    "        # Kerr (w16, w17) - Omitted for backend compatibility\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.c_layer1.parameters() + self.c_layer2.parameters() + self.q_weights\n",
    "    \n",
    "# --- 4. Training ---\n",
    "def get_full_data(n_samples=1000):\n",
    "    df = pd.read_csv(r\"D:\\Academic\\QML_Intern\\Anomaly_Detection\\Fraud_Detection\\creditcard_data.csv\")\n",
    "    fraud = df[df['Class'] == 1]\n",
    "    normal = df[df['Class'] == 0]\n",
    "    n_per = n_samples // 2\n",
    "    fraud = resample(fraud, n_samples=n_per, random_state=42)\n",
    "    normal = resample(normal, n_samples=n_per, random_state=42)\n",
    "    df_bal = pd.concat([fraud, normal])\n",
    "    \n",
    "    X = df_bal.drop(['Class', 'Time'], axis=1).values\n",
    "    y = df_bal['Class'].values\n",
    "    \n",
    "    # Scale is crucial\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup\n",
    "X_train, X_test, y_train, y_test = get_full_data(n_samples=10000)\n",
    "model = HybridFraudDetector(n_features=29)\n",
    "optimizer_params = model.parameters()\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.1 # Aggressive learning rate to break symmetry\n",
    "\n",
    "print(f\"Training with MEAN PHOTON metric...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Mini-batch 8\n",
    "    for i in range(0, len(X_train), 64):\n",
    "        batch_idx = indices[i:i+64]\n",
    "        batch_loss = Value(0)\n",
    "        \n",
    "        # Inside your training loop...\n",
    "        for idx in batch_idx:\n",
    "            # pred range is [-1.0, +1.0]\n",
    "            pred = model.forward(X_train[idx])\n",
    "            y_target = y_train[idx]\n",
    "            \n",
    "            # --- Robust MSE (Fidelity Loss) ---\n",
    "            if y_target == 1:\n",
    "                # FRAUD: We want pred to be +1.0\n",
    "                # Loss = (1 - pred)^2\n",
    "                diff = Value(1) + (pred * Value(-1))\n",
    "                term = diff * diff\n",
    "            else:\n",
    "                # NORMAL: We want pred to be -1.0\n",
    "                # Loss = (-1 - pred)^2  -> equivalent to (1 + pred)^2\n",
    "                diff = Value(1) + pred\n",
    "                term = diff * diff\n",
    "                \n",
    "            batch_loss = batch_loss + term\n",
    "            \n",
    "        batch_loss = batch_loss * Value(1.0/64)\n",
    "        \n",
    "        # Zero Grads\n",
    "        for p in optimizer_params: p.zero_grad()\n",
    "        \n",
    "        # Backprop\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # Update\n",
    "        for p in optimizer_params:\n",
    "            p.data -= lr * p.grad\n",
    "            \n",
    "        epoch_loss += batch_loss.data\n",
    "        print(f\"\\rBatch {i//64} | Loss: {batch_loss.data:.4f}\", end=\"\")\n",
    "        \n",
    "    print(f\"\\nEpoch {epoch+1} | Avg Loss: {epoch_loss / (len(X_train)/64):.4f} | Time: {time.time()-start:.1f}s\")\n",
    "    \n",
    "    # Test AUC\n",
    "    preds = [model.forward(x).data for x in X_test]\n",
    "    try:\n",
    "        print(f\"Test AUC: {roc_auc_score(y_test, preds):.4f}\")\n",
    "    except: pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boson_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
