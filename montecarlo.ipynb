{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed1d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98955f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayeredHeisenbergMC:\n",
    "    def __init__(self, L=10, Lz=10, p_weak=0.0):\n",
    "        self.L = L\n",
    "        self.Lz = Lz\n",
    "        self.N = L * L * Lz\n",
    "        \n",
    "        # Initialize Spins: Random 3D unit vectors\n",
    "        # Shape: (L, L, Lz, 3)\n",
    "        spins = np.random.randn(L, L, Lz, 3)\n",
    "        norms = np.linalg.norm(spins, axis=3, keepdims=True)\n",
    "        self.spins = spins / norms\n",
    "        \n",
    "        # --- LAYERED DISORDER SETUP (Eq. 2 in Paper) ---\n",
    "        # J_para depends only on Z layer index\n",
    "        # Strong Layer (J=1.0), Weak Layer (J=0.2)\n",
    "        # p_weak is the concentration of weak layers (p in the paper)\n",
    "        self.J_para = np.ones(Lz) \n",
    "        self.J_perp = np.ones(Lz) # Inter-layer coupling usually constant or correlated\n",
    "        \n",
    "        if p_weak > 0:\n",
    "            # Randomly assign layers as \"Weak\"\n",
    "            is_weak = np.random.rand(Lz) < p_weak\n",
    "            self.J_para[is_weak] = 0.2  # Weak intra-layer coupling\n",
    "            # The paper mentions J_perp can also be random, \n",
    "            # but usually J_para is the dominant effect for layered physics.\n",
    "            \n",
    "    def energy(self):\n",
    "        \"\"\"Calculate Total Energy (Hamiltonian Eq. 1)\"\"\"\n",
    "        E = 0\n",
    "        \n",
    "        # 1. Intra-layer interactions (neighbors in x and y)\n",
    "        # We vectorise this: S(r) dot S(r+x)\n",
    "        Sx = np.sum(self.spins * np.roll(self.spins, 1, axis=0), axis=3)\n",
    "        Sy = np.sum(self.spins * np.roll(self.spins, 1, axis=1), axis=3)\n",
    "        \n",
    "        # Multiply by J_para which varies by Z layer\n",
    "        # J_para shape (Lz,) needs broadcasting to (L, L, Lz)\n",
    "        J_broadcast = self.J_para[np.newaxis, np.newaxis, :]\n",
    "        E -= np.sum(J_broadcast * (Sx + Sy))\n",
    "        \n",
    "        # 2. Inter-layer interactions (neighbors in z)\n",
    "        Sz = np.sum(self.spins * np.roll(self.spins, 1, axis=2), axis=3)\n",
    "        J_perp_broadcast = self.J_perp[np.newaxis, np.newaxis, :]\n",
    "        E -= np.sum(J_perp_broadcast * Sz)\n",
    "        \n",
    "        return E\n",
    "\n",
    "    def mc_step(self, T, delta=0.2):\n",
    "        \"\"\"Metropolis Step for Heisenberg Spins\"\"\"\n",
    "        # We cannot just flip spins; we rotate them randomly.\n",
    "        # Pick random sites\n",
    "        for _ in range(self.N):\n",
    "            x, y, z = np.random.randint(0, self.L), np.random.randint(0, self.L), np.random.randint(0, self.Lz)\n",
    "            \n",
    "            old_spin = self.spins[x, y, z].copy()\n",
    "            \n",
    "            # Propose new spin: add small random vector and re-normalize\n",
    "            perturbation = np.random.uniform(-delta, delta, 3)\n",
    "            new_spin = old_spin + perturbation\n",
    "            new_spin /= np.linalg.norm(new_spin)\n",
    "            \n",
    "            # Calculate local energy change\n",
    "            # Neighbors\n",
    "            nbrs = (\n",
    "                self.spins[(x+1)%self.L, y, z] + self.spins[(x-1)%self.L, y, z] +\n",
    "                self.spins[x, (y+1)%self.L, z] + self.spins[x, (y-1)%self.L, z]\n",
    "            )\n",
    "            # Z-neighbors need specific J_perp handling\n",
    "            # (Simplified for clarity: assuming J_perp constant for local update calculation)\n",
    "            nbr_z_up = self.spins[x, y, (z+1)%self.Lz]\n",
    "            nbr_z_down = self.spins[x, y, (z-1)%self.Lz]\n",
    "            \n",
    "            # Local H_eff\n",
    "            # Note: Explicit index checking needed for varying J\n",
    "            J_local = self.J_para[z]\n",
    "            dE_interaction = -J_local * np.dot(new_spin - old_spin, nbrs)\n",
    "            dE_z = -1.0 * np.dot(new_spin - old_spin, nbr_z_up + nbr_z_down) # Assuming J_perp=1\n",
    "            \n",
    "            dE = dE_interaction + dE_z\n",
    "\n",
    "            if dE < 0 or np.random.rand() < np.exp(-dE / T):\n",
    "                self.spins[x, y, z] = new_spin\n",
    "                \n",
    "    def simulate(self, T, steps=500):\n",
    "        # Equilibration\n",
    "        for _ in range(200):\n",
    "            self.mc_step(T)\n",
    "            \n",
    "        data = []\n",
    "        for _ in range(steps):\n",
    "            self.mc_step(T)\n",
    "            data.append(self.spins.copy())\n",
    "        return np.array(data)\n",
    "\n",
    "# --- Data Generation Strategy ---\n",
    "# 1. Pure System (p_weak=0.0) -> Label 0 (Ordered) / Label 1 (Disordered)\n",
    "# 2. Layered System (p_weak=0.5) -> Test Data to find Griffiths Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564d2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Input shape: (L, L, Lz, 3)\n",
    "# 3 channels correspond to Sx, Sy, Sz\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(10, 10, 10, 3)),\n",
    "    \n",
    "    # 3D Convolution to catch spatial correlations in all directions\n",
    "    layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "    layers.MaxPooling3D((2, 2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax') # [P(Ordered), P(Disordered)]\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13366619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 20 samples...\n",
      "  > Simulating Ordered Phase (T=0.1)...\n",
      "  > Simulating Disordered Phase (T=10.0)...\n",
      "\n",
      "Data Shape: (20, 10, 10, 10, 3)\n",
      "Labels Shape: (20,)\n",
      "\n",
      "Starting Training...\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 3s 364ms/step - loss: 0.5609 - accuracy: 0.6250 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2294 - accuracy: 0.8750 - val_loss: 0.4426 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1383 - accuracy: 0.9375 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "\n",
      "Training Complete! TensorBoard/Graphs can be plotted now.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. EXECUTION: Generate Data and Train ---\n",
    "\n",
    "def generate_dataset(n_samples=20):\n",
    "    \"\"\"\n",
    "    Generates a balanced dataset of Ordered (Low T) vs Disordered (High T)\n",
    "    n_samples: Total samples (half ordered, half disordered)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"Generating {n_samples} samples...\")\n",
    "    \n",
    "    # Half Ordered (Low Temp, T=0.1)\n",
    "    print(\"  > Simulating Ordered Phase (T=0.1)...\")\n",
    "    for i in range(n_samples // 2):\n",
    "        # Initialize system (p_weak=0 for pure system training)\n",
    "        sim = LayeredHeisenbergMC(L=10, Lz=10, p_weak=0.0)\n",
    "        \n",
    "        # Run simulation (reduced steps for speed during testing)\n",
    "        # We take the FINAL state of the simulation as one data point\n",
    "        traj = sim.simulate(T=0.1, steps=100) \n",
    "        final_state = traj[-1]  # Shape (10, 10, 10, 3)\n",
    "        \n",
    "        X.append(final_state)\n",
    "        y.append(0) # Label 0 = Ordered\n",
    "\n",
    "    # Half Disordered (High Temp, T=10.0)\n",
    "    print(\"  > Simulating Disordered Phase (T=10.0)...\")\n",
    "    for i in range(n_samples // 2):\n",
    "        sim = LayeredHeisenbergMC(L=10, Lz=10, p_weak=0.0)\n",
    "        \n",
    "        traj = sim.simulate(T=10.0, steps=100)\n",
    "        final_state = traj[-1]\n",
    "        \n",
    "        X.append(final_state)\n",
    "        y.append(1) # Label 1 = Disordered\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 1. Generate the data\n",
    "X_train, y_train = generate_dataset(n_samples=20) # Small batch for testing\n",
    "\n",
    "# 2. Check shapes\n",
    "print(f\"\\nData Shape: {X_train.shape}\") # Should be (20, 10, 10, 10, 3)\n",
    "print(f\"Labels Shape: {y_train.shape}\")\n",
    "\n",
    "# 3. Train the Model\n",
    "print(\"\\nStarting Training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=4,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Complete! TensorBoard/Graphs can be plotted now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aec2ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is using this Python:\n",
      "c:\\Users\\PARAM\\Documents\\Gemini-Test\\DDP\\boson_env\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Notebook is using this Python:\")\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8217bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_large_dataset(n_samples=10000, L=10):\n",
    "    \"\"\"\n",
    "    Generates synthetic 3D Heisenberg spin configurations for 4 classes.\n",
    "    L: Linear size of the lattice (L x L x L)\n",
    "    n_samples: Total number of samples to generate\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    samples_per_class = n_samples // 4\n",
    "    print(f\"Generating {n_samples} samples ({samples_per_class} per class)...\")\n",
    "    \n",
    "    # --- CLASS 0: STRONGLY DISORDERED (SD) ---\n",
    "    # Physics: High Temp (T > Tu). Pure Paramagnetic.\n",
    "    # Visual: Completely random 3D vectors.\n",
    "    print(\"Generating Class 0: Strongly Disordered...\")\n",
    "    for _ in range(samples_per_class):\n",
    "        # Generate random 3D vectors\n",
    "        config = np.random.randn(L, L, L, 3)\n",
    "        # Normalize to unit length (Heisenberg spins)\n",
    "        config /= np.linalg.norm(config, axis=3, keepdims=True)\n",
    "        X.append(config)\n",
    "        Y.append(0)\n",
    "\n",
    "    # --- CLASS 1: WEAKLY DISORDERED (WD) - GRIFFITHS PHASE ---\n",
    "    # Physics: Tu > T > Tc. Bulk is Disordered, but Rare Strong Slabs are Ordered.\n",
    "    # Visual: Random noise + A thick \"slab\" of aligned spins.\n",
    "    print(\"Generating Class 1: Weakly Disordered (Griffiths)...\")\n",
    "    for _ in range(samples_per_class):\n",
    "        config = np.random.randn(L, L, L, 3)\n",
    "        config /= np.linalg.norm(config, axis=3, keepdims=True)\n",
    "        \n",
    "        # Create a \"Rare Strong Slab\" (Thickness 3 to 5 layers)\n",
    "        slab_thickness = np.random.randint(3, 6)\n",
    "        z_start = np.random.randint(0, L - slab_thickness)\n",
    "        \n",
    "        # Pick a random direction for the slab's order\n",
    "        direction = np.random.randn(3)\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        \n",
    "        # Apply order to the slab (with small thermal noise)\n",
    "        slab_noise = np.random.normal(0, 0.2, (L, L, slab_thickness, 3))\n",
    "        config[:, :, z_start:z_start+slab_thickness, :] = direction + slab_noise\n",
    "        \n",
    "        # Re-normalize the slab part\n",
    "        # (We only normalize the slice we modified to keep it physical)\n",
    "        slice_norm = np.linalg.norm(config[:, :, z_start:z_start+slab_thickness, :], axis=3, keepdims=True)\n",
    "        config[:, :, z_start:z_start+slab_thickness, :] /= slice_norm\n",
    "        \n",
    "        X.append(config)\n",
    "        Y.append(1)\n",
    "\n",
    "    # --- CLASS 2: WEAKLY ORDERED (WO) - GRIFFITHS PHASE ---\n",
    "    # Physics: Tc > T > Tl. Bulk is Ordered, but Rare Weak Slabs are Disordered.\n",
    "    # Visual: Aligned spins + A thick \"slab\" of random noise.\n",
    "    print(\"Generating Class 2: Weakly Ordered (Griffiths)...\")\n",
    "    for _ in range(samples_per_class):\n",
    "        # Start with global order\n",
    "        direction = np.random.randn(3)\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        \n",
    "        # Create base config with thermal noise\n",
    "        config = np.ones((L, L, L, 3)) * direction\n",
    "        config += np.random.normal(0, 0.3, (L, L, L, 3))\n",
    "        config /= np.linalg.norm(config, axis=3, keepdims=True)\n",
    "        \n",
    "        # Create a \"Rare Weak Slab\" (Disordered Region)\n",
    "        slab_thickness = np.random.randint(3, 6)\n",
    "        z_start = np.random.randint(0, L - slab_thickness)\n",
    "        \n",
    "        # Generate noise for the slab\n",
    "        noise_slab = np.random.randn(L, L, slab_thickness, 3)\n",
    "        noise_slab /= np.linalg.norm(noise_slab, axis=3, keepdims=True)\n",
    "        \n",
    "        # Insert the disordered slab\n",
    "        config[:, :, z_start:z_start+slab_thickness, :] = noise_slab\n",
    "        \n",
    "        X.append(config)\n",
    "        Y.append(2)\n",
    "\n",
    "    # --- CLASS 3: STRONGLY ORDERED (SO) ---\n",
    "    # Physics: T < Tl. Pure Ferromagnetic.\n",
    "    # Visual: Almost perfectly aligned spins (very low noise).\n",
    "    print(\"Generating Class 3: Strongly Ordered...\")\n",
    "    for _ in range(samples_per_class):\n",
    "        direction = np.random.randn(3)\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        \n",
    "        config = np.ones((L, L, L, 3)) * direction\n",
    "        # Very low thermal noise\n",
    "        config += np.random.normal(0, 0.1, (L, L, L, 3))\n",
    "        config /= np.linalg.norm(config, axis=3, keepdims=True)\n",
    "        \n",
    "        X.append(config)\n",
    "        Y.append(3)\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf27a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 samples (2500 per class)...\n",
      "Generating Class 0: Strongly Disordered...\n",
      "Generating Class 1: Weakly Disordered (Griffiths)...\n",
      "Generating Class 2: Weakly Ordered (Griffiths)...\n",
      "Generating Class 3: Strongly Ordered...\n",
      "Training Data Shape: (8000, 10, 10, 10, 3)\n",
      "Testing Data Shape: (2000, 10, 10, 10, 3)\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 44s 344ms/step - loss: 0.2165 - accuracy: 0.9168 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 1.4535e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 5.5460e-04 - accuracy: 1.0000 - val_loss: 7.8340e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 42s 336ms/step - loss: 4.3568e-04 - accuracy: 1.0000 - val_loss: 1.6636e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 42s 333ms/step - loss: 4.8851e-04 - accuracy: 1.0000 - val_loss: 2.9471e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.0648 - accuracy: 0.9804 - val_loss: 0.0082 - val_accuracy: 0.9955\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 4.7835e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 1.5134e-04 - accuracy: 1.0000 - val_loss: 2.4905e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 32s 259ms/step - loss: 1.2549e-04 - accuracy: 1.0000 - val_loss: 6.1921e-06 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 37s 292ms/step - loss: 8.1846e-05 - accuracy: 1.0000 - val_loss: 4.9027e-06 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 32s 258ms/step - loss: 7.4555e-05 - accuracy: 1.0000 - val_loss: 3.3815e-06 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 6.6674e-05 - accuracy: 1.0000 - val_loss: 3.5916e-06 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 32s 259ms/step - loss: 3.0256e-05 - accuracy: 1.0000 - val_loss: 2.6023e-06 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 3.5714e-05 - accuracy: 1.0000 - val_loss: 1.7360e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 3.7748e-05 - accuracy: 1.0000 - val_loss: 1.9687e-06 - val_accuracy: 1.0000\n",
      "Model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate Data\n",
    "L_SIZE = 10\n",
    "N_SAMPLES = 10000  # Large dataset\n",
    "\n",
    "X, Y = generate_large_dataset(n_samples=N_SAMPLES, L=L_SIZE)\n",
    "\n",
    "# Split into Train and Test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}\")\n",
    "\n",
    "# 2. Define the 3D CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(L_SIZE, L_SIZE, L_SIZE, 3)),\n",
    "    \n",
    "    # First Conv Block\n",
    "    layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling3D((2, 2, 2)),\n",
    "    \n",
    "    # Second Conv Block (Added for better feature extraction on larger data)\n",
    "    layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "    # Note: Depending on L_SIZE, a second MaxPooling might reduce dimensions too much.\n",
    "    # For L=10, MaxPool (2,2,2) -> (5,5,5). Another MaxPool would be too small.\n",
    "    # So we skip the second pooling or use stride 1.\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Dense Layers\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3), # Dropout to prevent overfitting on synthetic patterns\n",
    "    layers.Dense(4, activation='softmax') # 4 Output Neurons\n",
    "])\n",
    "\n",
    "# 3. Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Train\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=15,            # Increased epochs for larger data\n",
    "    batch_size=64,        # Batch processing\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "\n",
    "# 5. Save the Model\n",
    "model.save('heisenberg_griffiths_detector.keras')\n",
    "print(\"Model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1add36e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, ..., 2, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00757819",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     17\u001b[0m         sim \u001b[38;5;241m=\u001b[39m LayeredHeisenbergMC(L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, Lz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, p_weak\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m         traj \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m         final_state \u001b[38;5;241m=\u001b[39m traj[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m         X\u001b[38;5;241m.\u001b[39mappend(final_state)\n",
      "Cell \u001b[1;32mIn[8], line 87\u001b[0m, in \u001b[0;36mLayeredHeisenbergMC.simulate\u001b[1;34m(self, T, steps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, T, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Equilibration\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m---> 87\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmc_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m, in \u001b[0;36mLayeredHeisenbergMC.mc_step\u001b[1;34m(self, T, delta)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Local H_eff\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Note: Explicit index checking needed for varying J\u001b[39;00m\n\u001b[0;32m     75\u001b[0m J_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_para[z]\n\u001b[1;32m---> 76\u001b[0m dE_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mJ_local \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_spin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mold_spin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m dE_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(new_spin \u001b[38;5;241m-\u001b[39m old_spin, nbr_z_up \u001b[38;5;241m+\u001b[39m nbr_z_down) \u001b[38;5;66;03m# Assuming J_perp=1\u001b[39;00m\n\u001b[0;32m     79\u001b[0m dE \u001b[38;5;241m=\u001b[39m dE_interaction \u001b[38;5;241m+\u001b[39m dE_z\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def detect_phases(model, monte_carlo_data):\n",
    "    \"\"\"\n",
    "    monte_carlo_data: Shape (N_temperature_steps, L, L, L, 3)\n",
    "    Returns: Probabilities for [SD, WD, WO, SO] at each temperature\n",
    "    \"\"\"\n",
    "    predictions = model.predict(monte_carlo_data)\n",
    "    return predictions\n",
    "\n",
    "# Example Usage (Conceptual):\n",
    "# 1. Run MC Simulation across temperatures T = 4.0 down to 0.1\n",
    "n_samples = 2000\n",
    "t_lis = np.linspace(4.0, 0.1, num=20) # 20 temperature points\n",
    "X = []\n",
    "Y = []  \n",
    "for t in t_lis:\n",
    "    for i in range(n_samples // 2):\n",
    "            sim = LayeredHeisenbergMC(L=10, Lz=10, p_weak=0.0)\n",
    "            \n",
    "            traj = sim.simulate(T=t, steps=100)\n",
    "            final_state = traj[-1]\n",
    "            \n",
    "            X.append(final_state)\n",
    "            if t > 3.0: \n",
    "                Y.append(0) # Label 1 = Disordered\n",
    "            elif t > 1.5:\n",
    "                Y.append(1) # Label 2 = Weakly Disordered\n",
    "            elif t > 0.5:\n",
    "                Y.append(2) # Label 3 = Weakly Ordered\n",
    "            else:\n",
    "                Y.append(3) # Label 4 = Strongly Ordered\n",
    "            \n",
    "\n",
    "# 2. Get predictions\n",
    "probs = detect_phases(model, np.array(X))\n",
    "err = np.abs(probs.argmax(axis=1) - np.array(Y))\n",
    "print(f\"Overall Accuracy: {(err == 0).mean() * 100:.2f}%\")\n",
    "# 3. Plotting the Griffiths Phase\n",
    "# Plot probs[:, 1] (Weakly Disordered) vs Temperature.\n",
    "# You should see a peak or a plateau in the Griffiths region (Tc < T < Tu).\n",
    "plt.plot(t_lis, probs[:, 1], label='Weakly Disordered')\n",
    "plt.plot(t_lis, probs[:, 0], label='Strongly Disordered')\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Probability of Weak Phases\")\n",
    "plt.title(\"Griffiths Phase\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
